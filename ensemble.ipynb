{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/COMS4771')\n",
        "DATA_PATH = Path.cwd() / 'data'\n",
        "RAW = DATA_PATH / 'raw'\n",
        "PROCESSED = DATA_PATH / 'processed'\n",
        "SUBMISSION = DATA_PATH / 'submission'\n",
        "\n",
        "from utils import *"
      ],
      "metadata": {
        "id": "y3mZW8cTdHFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seeds = [64, 102, 202, 302, 512, 1024, 2024, 128, 802, 902]\n",
        "\n",
        "i = 1\n",
        "for seed in random_seeds:\n",
        "    try:\n",
        "        shutil.rmtree('models')\n",
        "    except OSError as e:\n",
        "        print (\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
        "\n",
        "    os.mkdir('models')\n",
        "\n",
        "    xdev, ydev, xtest = load_data(preprocess_flag=True)\n",
        "    xtrain, ytrain, xval, yval, xtest = process_and_split(xdev=xdev, ydev=ydev, xtest=xtest, random_seed=seed)\n",
        "\n",
        "    input_size = xtrain.shape[1]\n",
        "    output_size = 1\n",
        "\n",
        "    EPOCHS = 100\n",
        "    batch_size = 764\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    train_loader, valid_loader = make_data_loader(xtrain=xtrain, ytrain=ytrain, xval=xval, yval=yval, batch_size=batch_size)\n",
        "    \n",
        "    model = build_model(input_size=input_size, output_size=1)\n",
        "\n",
        "    print(f'\\n\\nProcessing seed: {i}/{10}\\n\\n')\n",
        "    valid_loss = train(model=model, train_loader=train_loader, valid_loader=valid_loader, learning_rate=learning_rate, EPOCHS=EPOCHS, batch_size=batch_size, train_shape=xtrain.shape[0])\n",
        "\n",
        "    file_name = f'ff_{i}.csv'\n",
        "    ytest = predict(model=model, xtest=xtest, valid_loss=valid_loss, file_name=file_name)\n",
        "    i += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfouYsrWif88",
        "outputId": "ff9d09f5-2eb7-4095-de80-016e326d41e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Processing seed: 1/10\n",
            "\n",
            "\n",
            "EPOCH:1/100 - Training Loss: 402.175869488773, Validation Loss: 345.9002483549572\n",
            "EPOCH:2/100 - Training Loss: 351.79382356994194, Validation Loss: 328.77528424944194\n",
            "EPOCH:3/100 - Training Loss: 346.9573288145953, Validation Loss: 321.7416996547154\n",
            "EPOCH:4/100 - Training Loss: 341.8408741006419, Validation Loss: 322.12405642554876\n",
            "EPOCH:5/100 - Training Loss: 339.5811480610923, Validation Loss: 320.3013435000465\n",
            "EPOCH:6/100 - Training Loss: 337.52684538119735, Validation Loss: 319.139953031994\n",
            "EPOCH:7/100 - Training Loss: 336.05298197525497, Validation Loss: 318.3755437941778\n",
            "EPOCH:8/100 - Training Loss: 332.9037114691905, Validation Loss: 321.05441095261347\n",
            "EPOCH:9/100 - Training Loss: 332.3155123544479, Validation Loss: 317.81788446335565\n",
            "EPOCH:10/100 - Training Loss: 330.56265353473674, Validation Loss: 319.37373526436943\n",
            "EPOCH:11/100 - Training Loss: 328.7152670630407, Validation Loss: 317.50487452915735\n",
            "EPOCH:12/100 - Training Loss: 327.5830222701116, Validation Loss: 321.3239788237072\n",
            "EPOCH:13/100 - Training Loss: 326.46428241593173, Validation Loss: 321.53237595331103\n",
            "EPOCH:14/100 - Training Loss: 325.25724916298805, Validation Loss: 319.4266854422433\n",
            "EPOCH:15/100 - Training Loss: 324.00726627905215, Validation Loss: 320.4849340529669\n",
            "EPOCH:16/100 - Training Loss: 322.37492785761066, Validation Loss: 320.7400108700707\n",
            "EPOCH:17/100 - Training Loss: 321.7803119668528, Validation Loss: 326.7786359514509\n",
            "EPOCH:18/100 - Training Loss: 321.20139190746664, Validation Loss: 318.2946591331845\n",
            "EPOCH:19/100 - Training Loss: 319.7721488060416, Validation Loss: 318.96153622581846\n",
            "EPOCH:20/100 - Training Loss: 318.4126085813973, Validation Loss: 320.6741994222005\n",
            "EPOCH:21/100 - Training Loss: 318.87668105241505, Validation Loss: 319.2406472342355\n",
            "EPOCH:22/100 - Training Loss: 316.75401579281026, Validation Loss: 320.9198769705636\n",
            "EPOCH:23/100 - Training Loss: 315.9685045847745, Validation Loss: 321.0829111735026\n",
            "EPOCH:24/100 - Training Loss: 315.2235820833994, Validation Loss: 320.36386137462796\n",
            "EPOCH:25/100 - Training Loss: 314.4804042552138, Validation Loss: 321.4251460484096\n",
            "EPOCH:26/100 - Training Loss: 313.3083071469692, Validation Loss: 320.9626220703125\n",
            "EPOCH:27/100 - Training Loss: 311.68008513894455, Validation Loss: 322.03170790899367\n",
            "EPOCH:28/100 - Training Loss: 312.12275571982445, Validation Loss: 321.97443426223026\n",
            "EPOCH:29/100 - Training Loss: 310.5893879528546, Validation Loss: 322.5922103155227\n",
            "EPOCH:30/100 - Training Loss: 309.98121566590373, Validation Loss: 321.00433829171317\n",
            "EPOCH:31/100 - Training Loss: 308.8125167154754, Validation Loss: 321.36449148995536\n",
            "EPOCH:32/100 - Training Loss: 308.4508966887481, Validation Loss: 324.5941426595052\n",
            "EPOCH:33/100 - Training Loss: 307.01675822911227, Validation Loss: 322.6620926629929\n",
            "EPOCH:34/100 - Training Loss: 306.0799929452682, Validation Loss: 322.7405293782552\n",
            "EPOCH:35/100 - Training Loss: 305.9720042736263, Validation Loss: 324.2543702625093\n",
            "EPOCH:36/100 - Training Loss: 305.3408407789426, Validation Loss: 322.73976222446987\n",
            "EPOCH:37/100 - Training Loss: 304.09488322911227, Validation Loss: 322.356146530878\n",
            "EPOCH:38/100 - Training Loss: 302.4575340616959, Validation Loss: 323.21021684919083\n",
            "EPOCH:39/100 - Training Loss: 301.93126851448295, Validation Loss: 323.2893981933594\n",
            "EPOCH:40/100 - Training Loss: 300.70764502477533, Validation Loss: 326.318993414016\n",
            "EPOCH:41/100 - Training Loss: 299.9826368818989, Validation Loss: 330.5935166131882\n",
            "EPOCH:42/100 - Training Loss: 299.2944273482075, Validation Loss: 323.0772984095982\n",
            "EPOCH:43/100 - Training Loss: 297.72149184780073, Validation Loss: 327.17972397577194\n",
            "EPOCH:44/100 - Training Loss: 296.20077336204366, Validation Loss: 322.9299803234282\n",
            "EPOCH:45/100 - Training Loss: 296.295430822987, Validation Loss: 339.3080043247768\n",
            "EPOCH:46/100 - Training Loss: 295.24123863274843, Validation Loss: 324.233145577567\n",
            "EPOCH:47/100 - Training Loss: 293.0568347830761, Validation Loss: 324.32823965890066\n",
            "EPOCH:48/100 - Training Loss: 292.7965501043279, Validation Loss: 323.343896484375\n",
            "EPOCH:49/100 - Training Loss: 291.40308507587, Validation Loss: 325.3818658737909\n",
            "EPOCH:50/100 - Training Loss: 291.19887402518555, Validation Loss: 325.8778118315197\n",
            "EPOCH:51/100 - Training Loss: 289.8586236776202, Validation Loss: 329.87071344284783\n",
            "EPOCH:52/100 - Training Loss: 288.68276777244694, Validation Loss: 325.8629340762184\n",
            "EPOCH:53/100 - Training Loss: 287.6530065058524, Validation Loss: 324.7004099527995\n",
            "EPOCH:54/100 - Training Loss: 287.0820775179988, Validation Loss: 324.81511361258373\n",
            "EPOCH:55/100 - Training Loss: 285.9826902330349, Validation Loss: 324.7456868489583\n",
            "EPOCH:56/100 - Training Loss: 284.06204737114734, Validation Loss: 324.84830104282923\n",
            "EPOCH:57/100 - Training Loss: 283.79935185835296, Validation Loss: 325.6858110700335\n",
            "EPOCH:58/100 - Training Loss: 283.2793642103245, Validation Loss: 328.83616449265253\n",
            "EPOCH:59/100 - Training Loss: 281.0885893063784, Validation Loss: 327.49757225399924\n",
            "EPOCH:60/100 - Training Loss: 281.7940830421903, Validation Loss: 329.66862051827565\n",
            "EPOCH:61/100 - Training Loss: 281.41075114247906, Validation Loss: 326.3014391217913\n",
            "EPOCH:62/100 - Training Loss: 281.2436830434139, Validation Loss: 324.9663584391276\n",
            "EPOCH:63/100 - Training Loss: 279.9577695168288, Validation Loss: 324.7887885684059\n",
            "EPOCH:64/100 - Training Loss: 280.33613662901814, Validation Loss: 328.47428632463726\n",
            "EPOCH:65/100 - Training Loss: 278.70234885932723, Validation Loss: 325.3758540562221\n",
            "EPOCH:66/100 - Training Loss: 277.8119076385134, Validation Loss: 324.6404721214658\n",
            "EPOCH:67/100 - Training Loss: 277.76131706875094, Validation Loss: 325.1480718703497\n",
            "EPOCH:68/100 - Training Loss: 275.785216647856, Validation Loss: 327.57928583054314\n",
            "EPOCH:69/100 - Training Loss: 276.64477970605816, Validation Loss: 327.7039219447545\n",
            "EPOCH:70/100 - Training Loss: 275.1694941008575, Validation Loss: 329.27731032598587\n",
            "EPOCH:71/100 - Training Loss: 274.5223556372886, Validation Loss: 326.46696137927825\n",
            "EPOCH:72/100 - Training Loss: 274.3634867520207, Validation Loss: 327.0751937139602\n",
            "EPOCH:73/100 - Training Loss: 273.3358637734643, Validation Loss: 327.07494303385414\n",
            "EPOCH:74/100 - Training Loss: 273.5385015483119, Validation Loss: 326.16602783203126\n",
            "EPOCH:75/100 - Training Loss: 272.7385500268322, Validation Loss: 328.03575599307106\n",
            "EPOCH:76/100 - Training Loss: 271.02201098530844, Validation Loss: 329.0682384672619\n",
            "EPOCH:77/100 - Training Loss: 270.6652029397095, Validation Loss: 330.1787426176525\n",
            "EPOCH:78/100 - Training Loss: 270.55055770327766, Validation Loss: 325.90127650669643\n",
            "EPOCH:79/100 - Training Loss: 270.09894822150255, Validation Loss: 327.65562816801526\n",
            "EPOCH:80/100 - Training Loss: 269.16401264491117, Validation Loss: 331.5419596354167\n",
            "EPOCH:81/100 - Training Loss: 269.0773314701344, Validation Loss: 330.9825012207031\n",
            "EPOCH:82/100 - Training Loss: 268.1073662113745, Validation Loss: 327.63180106026783\n",
            "EPOCH:83/100 - Training Loss: 267.09481875253465, Validation Loss: 326.30804835728236\n",
            "EPOCH:84/100 - Training Loss: 266.9188477145174, Validation Loss: 333.99297921316963\n",
            "EPOCH:85/100 - Training Loss: 265.4554147652055, Validation Loss: 332.61241745721725\n",
            "EPOCH:86/100 - Training Loss: 264.84123242726474, Validation Loss: 334.89129348028274\n",
            "EPOCH:87/100 - Training Loss: 264.4433603218461, Validation Loss: 331.5391805013021\n",
            "EPOCH:88/100 - Training Loss: 263.6544462217636, Validation Loss: 330.52490859258745\n",
            "EPOCH:89/100 - Training Loss: 263.09043389004, Validation Loss: 329.10486653645836\n",
            "EPOCH:90/100 - Training Loss: 262.43968277159627, Validation Loss: 328.3554928734189\n",
            "EPOCH:91/100 - Training Loss: 263.1956059676651, Validation Loss: 329.3351495651972\n",
            "EPOCH:92/100 - Training Loss: 261.4247637364062, Validation Loss: 330.2579177129836\n",
            "EPOCH:93/100 - Training Loss: 263.1740951538086, Validation Loss: 329.5375640869141\n",
            "EPOCH:94/100 - Training Loss: 260.4644286126112, Validation Loss: 329.1453280494327\n",
            "EPOCH:95/100 - Training Loss: 260.7566780873301, Validation Loss: 332.42784104120165\n",
            "EPOCH:96/100 - Training Loss: 259.96950279727474, Validation Loss: 330.00066862560453\n",
            "EPOCH:97/100 - Training Loss: 258.7500140206057, Validation Loss: 334.9903346470424\n",
            "EPOCH:98/100 - Training Loss: 257.51469862034327, Validation Loss: 331.6742030552455\n",
            "EPOCH:99/100 - Training Loss: 257.1207904132852, Validation Loss: 330.99180690220425\n",
            "EPOCH:100/100 - Training Loss: 254.05329341478733, Validation Loss: 339.1352249872117\n",
            "Best epoch is epoch: 11\n",
            "Predictions saved\n",
            "\n",
            "\n",
            "Processing seed: 2/10\n",
            "\n",
            "\n",
            "EPOCH:1/100 - Training Loss: 407.3113477975486, Validation Loss: 330.70745050339474\n",
            "EPOCH:2/100 - Training Loss: 355.5332279979187, Validation Loss: 320.8883374895368\n",
            "EPOCH:3/100 - Training Loss: 349.71198179841326, Validation Loss: 316.7275894891648\n",
            "EPOCH:4/100 - Training Loss: 343.6302607497623, Validation Loss: 317.6510932559059\n",
            "EPOCH:5/100 - Training Loss: 343.0139496286615, Validation Loss: 314.6426499139695\n",
            "EPOCH:6/100 - Training Loss: 339.2258093931794, Validation Loss: 313.768301827567\n",
            "EPOCH:7/100 - Training Loss: 337.4125892074694, Validation Loss: 313.76016322544643\n",
            "EPOCH:8/100 - Training Loss: 335.9487390996164, Validation Loss: 312.6685514904204\n",
            "EPOCH:9/100 - Training Loss: 334.1323693760255, Validation Loss: 320.3966817220052\n",
            "EPOCH:10/100 - Training Loss: 333.019136742931, Validation Loss: 311.40442621140255\n",
            "EPOCH:11/100 - Training Loss: 330.97359668667957, Validation Loss: 314.324619547526\n",
            "EPOCH:12/100 - Training Loss: 329.12340521983145, Validation Loss: 313.9651701427641\n",
            "EPOCH:13/100 - Training Loss: 327.7337306712294, Validation Loss: 311.7030059814453\n",
            "EPOCH:14/100 - Training Loss: 327.21743202664686, Validation Loss: 312.8181700206938\n",
            "EPOCH:15/100 - Training Loss: 325.8764661911848, Validation Loss: 317.593749273391\n",
            "EPOCH:16/100 - Training Loss: 324.5103054365281, Validation Loss: 313.31342410133\n",
            "EPOCH:17/100 - Training Loss: 323.8333068337816, Validation Loss: 315.4493694487072\n",
            "EPOCH:18/100 - Training Loss: 322.8968245840869, Validation Loss: 312.61665620349703\n",
            "EPOCH:19/100 - Training Loss: 321.78258195754165, Validation Loss: 312.762989734468\n",
            "EPOCH:20/100 - Training Loss: 321.136182434519, Validation Loss: 312.893935866583\n",
            "EPOCH:21/100 - Training Loss: 319.93341687186523, Validation Loss: 312.4048110235305\n",
            "EPOCH:22/100 - Training Loss: 319.2706943047644, Validation Loss: 314.88885381789436\n",
            "EPOCH:23/100 - Training Loss: 318.46624540998147, Validation Loss: 318.4780527750651\n",
            "EPOCH:24/100 - Training Loss: 317.2807734814919, Validation Loss: 315.660990687779\n",
            "EPOCH:25/100 - Training Loss: 316.4352092151141, Validation Loss: 326.82198442731584\n",
            "EPOCH:26/100 - Training Loss: 315.3697215514991, Validation Loss: 317.81024489629834\n",
            "EPOCH:27/100 - Training Loss: 314.94416022471427, Validation Loss: 316.9343799409412\n",
            "EPOCH:28/100 - Training Loss: 313.42005501612846, Validation Loss: 314.84000287737166\n",
            "EPOCH:29/100 - Training Loss: 313.30305375347274, Validation Loss: 317.11703156970793\n",
            "EPOCH:30/100 - Training Loss: 311.8243808791859, Validation Loss: 316.6392525809152\n",
            "EPOCH:31/100 - Training Loss: 311.2093263867362, Validation Loss: 315.33470676967073\n",
            "EPOCH:32/100 - Training Loss: 310.89240503424963, Validation Loss: 321.4804194859096\n",
            "EPOCH:33/100 - Training Loss: 309.57502992397843, Validation Loss: 316.37954799107143\n",
            "EPOCH:34/100 - Training Loss: 309.39517772738293, Validation Loss: 316.15423395066034\n",
            "EPOCH:35/100 - Training Loss: 307.51770693248665, Validation Loss: 316.27847973051524\n",
            "EPOCH:36/100 - Training Loss: 307.38807789294987, Validation Loss: 315.75106477283293\n",
            "EPOCH:37/100 - Training Loss: 306.1556283591186, Validation Loss: 316.2536105201358\n",
            "EPOCH:38/100 - Training Loss: 304.9528193507957, Validation Loss: 317.01219773065475\n",
            "EPOCH:39/100 - Training Loss: 304.59605633501224, Validation Loss: 317.4630424862816\n",
            "EPOCH:40/100 - Training Loss: 302.8448116693975, Validation Loss: 319.81346144903273\n",
            "EPOCH:41/100 - Training Loss: 300.6544004818136, Validation Loss: 322.3960946219308\n",
            "EPOCH:42/100 - Training Loss: 299.53680980746105, Validation Loss: 318.0756847563244\n",
            "EPOCH:43/100 - Training Loss: 298.998170219913, Validation Loss: 319.81701020740326\n",
            "EPOCH:44/100 - Training Loss: 297.9200321461534, Validation Loss: 317.6210924421038\n",
            "EPOCH:45/100 - Training Loss: 295.43100252356334, Validation Loss: 317.2870574951172\n",
            "EPOCH:46/100 - Training Loss: 296.3482369944133, Validation Loss: 323.3114420572917\n",
            "EPOCH:47/100 - Training Loss: 293.1973105273554, Validation Loss: 319.58065025692895\n",
            "EPOCH:48/100 - Training Loss: 292.83040844682864, Validation Loss: 318.32313232421876\n",
            "EPOCH:49/100 - Training Loss: 291.66376400676717, Validation Loss: 318.7857376825242\n",
            "EPOCH:50/100 - Training Loss: 291.41030942056517, Validation Loss: 321.0801978701637\n",
            "EPOCH:51/100 - Training Loss: 291.62824039868923, Validation Loss: 326.39085678827195\n",
            "EPOCH:52/100 - Training Loss: 288.41295552765837, Validation Loss: 320.6104431152344\n",
            "EPOCH:53/100 - Training Loss: 288.3169928211585, Validation Loss: 320.4863736107236\n",
            "EPOCH:54/100 - Training Loss: 288.0974816115204, Validation Loss: 320.600392078218\n",
            "EPOCH:55/100 - Training Loss: 284.67626344958467, Validation Loss: 319.9218765985398\n",
            "EPOCH:56/100 - Training Loss: 285.7959884971309, Validation Loss: 319.2791288829985\n",
            "EPOCH:57/100 - Training Loss: 284.39972834257065, Validation Loss: 326.9236050560361\n",
            "EPOCH:58/100 - Training Loss: 284.0473441258251, Validation Loss: 320.05276445661275\n",
            "EPOCH:59/100 - Training Loss: 283.44432004141066, Validation Loss: 324.81788286481583\n",
            "EPOCH:60/100 - Training Loss: 283.3762083212914, Validation Loss: 319.86809924897693\n",
            "EPOCH:61/100 - Training Loss: 282.4765162866269, Validation Loss: 320.83087652297246\n",
            "EPOCH:62/100 - Training Loss: 282.5483747860128, Validation Loss: 320.48848324730284\n",
            "EPOCH:63/100 - Training Loss: 281.452913070351, Validation Loss: 321.73993094308037\n",
            "EPOCH:64/100 - Training Loss: 282.2383566153078, Validation Loss: 321.24113289969307\n",
            "EPOCH:65/100 - Training Loss: 280.1368801326342, Validation Loss: 321.54993416922434\n",
            "EPOCH:66/100 - Training Loss: 278.4412839247674, Validation Loss: 322.4539056687128\n",
            "EPOCH:67/100 - Training Loss: 279.59046954243735, Validation Loss: 322.2586085728237\n",
            "EPOCH:68/100 - Training Loss: 277.7480110040997, Validation Loss: 321.58335978190104\n",
            "EPOCH:69/100 - Training Loss: 275.7287017166757, Validation Loss: 321.03116411481585\n",
            "EPOCH:70/100 - Training Loss: 277.6423142644841, Validation Loss: 320.22612740652903\n",
            "EPOCH:71/100 - Training Loss: 275.73343032072154, Validation Loss: 321.8471512567429\n",
            "EPOCH:72/100 - Training Loss: 275.78308642621823, Validation Loss: 323.5519054594494\n",
            "EPOCH:73/100 - Training Loss: 274.2825045346645, Validation Loss: 326.24554065522693\n",
            "EPOCH:74/100 - Training Loss: 273.3197463952841, Validation Loss: 323.7683872767857\n",
            "EPOCH:75/100 - Training Loss: 273.84459522402085, Validation Loss: 320.0201597667876\n",
            "EPOCH:76/100 - Training Loss: 272.05067442026797, Validation Loss: 324.0772226969401\n",
            "EPOCH:77/100 - Training Loss: 271.5587454092531, Validation Loss: 326.916023617699\n",
            "EPOCH:78/100 - Training Loss: 269.38290733027856, Validation Loss: 321.56049107142854\n",
            "EPOCH:79/100 - Training Loss: 270.0568373504857, Validation Loss: 323.2660103934152\n",
            "EPOCH:80/100 - Training Loss: 266.98999261969885, Validation Loss: 323.42303583054314\n",
            "EPOCH:81/100 - Training Loss: 268.55877376001035, Validation Loss: 326.1774219331287\n",
            "EPOCH:82/100 - Training Loss: 265.4322502846365, Validation Loss: 322.8237531389509\n",
            "EPOCH:83/100 - Training Loss: 266.51206531388095, Validation Loss: 323.4200241815476\n",
            "EPOCH:84/100 - Training Loss: 266.0247818757924, Validation Loss: 323.86909150623137\n",
            "EPOCH:85/100 - Training Loss: 264.5032712258132, Validation Loss: 322.59506036667597\n",
            "EPOCH:86/100 - Training Loss: 266.48028081925787, Validation Loss: 321.5830813453311\n",
            "EPOCH:87/100 - Training Loss: 262.7483708420346, Validation Loss: 326.5510989234561\n",
            "EPOCH:88/100 - Training Loss: 263.44158403856375, Validation Loss: 328.96434733072914\n",
            "EPOCH:89/100 - Training Loss: 262.3915444421882, Validation Loss: 325.6265937441871\n",
            "EPOCH:90/100 - Training Loss: 261.2598295371117, Validation Loss: 326.74520496186756\n",
            "EPOCH:91/100 - Training Loss: 261.56032857234834, Validation Loss: 324.03645499093193\n",
            "EPOCH:92/100 - Training Loss: 261.0636399298693, Validation Loss: 323.11580360049294\n",
            "EPOCH:93/100 - Training Loss: 258.2906643268886, Validation Loss: 332.0461422874814\n",
            "EPOCH:94/100 - Training Loss: 259.73674342626606, Validation Loss: 326.22375924246654\n",
            "EPOCH:95/100 - Training Loss: 257.24514459141113, Validation Loss: 326.3478467668806\n",
            "EPOCH:96/100 - Training Loss: 258.40539652749294, Validation Loss: 325.64795880998884\n",
            "EPOCH:97/100 - Training Loss: 256.3391522428016, Validation Loss: 331.18355959937685\n",
            "EPOCH:98/100 - Training Loss: 257.57803280996535, Validation Loss: 325.5932903471447\n",
            "EPOCH:99/100 - Training Loss: 255.61165458660992, Validation Loss: 325.26455848330545\n",
            "EPOCH:100/100 - Training Loss: 253.49869927017102, Validation Loss: 325.5476739792597\n",
            "Best epoch is epoch: 10\n",
            "Predictions saved\n",
            "\n",
            "\n",
            "Processing seed: 3/10\n",
            "\n",
            "\n",
            "EPOCH:1/100 - Training Loss: 403.72635029722227, Validation Loss: 354.01423717680433\n",
            "EPOCH:2/100 - Training Loss: 351.84535967478604, Validation Loss: 339.15047665550594\n",
            "EPOCH:3/100 - Training Loss: 345.4072713920211, Validation Loss: 334.7944561186291\n",
            "EPOCH:4/100 - Training Loss: 340.916255140646, Validation Loss: 338.48411778041293\n",
            "EPOCH:5/100 - Training Loss: 338.3731832208383, Validation Loss: 333.6078587123326\n",
            "EPOCH:6/100 - Training Loss: 335.28300953138984, Validation Loss: 339.6347577776228\n",
            "EPOCH:7/100 - Training Loss: 333.1798019227094, Validation Loss: 332.2879629952567\n",
            "EPOCH:8/100 - Training Loss: 331.9885351504233, Validation Loss: 334.99495791480655\n",
            "EPOCH:9/100 - Training Loss: 330.3796324809582, Validation Loss: 334.57522917247957\n",
            "EPOCH:10/100 - Training Loss: 329.52839001493976, Validation Loss: 341.79945925758\n",
            "EPOCH:11/100 - Training Loss: 326.9208271327053, Validation Loss: 331.5911923363095\n",
            "EPOCH:12/100 - Training Loss: 325.48417029915674, Validation Loss: 333.7209681919643\n",
            "EPOCH:13/100 - Training Loss: 324.5453881967039, Validation Loss: 331.25542311895464\n",
            "EPOCH:14/100 - Training Loss: 323.27271898947924, Validation Loss: 332.0950443812779\n",
            "EPOCH:15/100 - Training Loss: 321.7538742029013, Validation Loss: 333.26485653831844\n",
            "EPOCH:16/100 - Training Loss: 321.4826157235303, Validation Loss: 334.12596188499816\n",
            "EPOCH:17/100 - Training Loss: 319.6187275451806, Validation Loss: 332.2970286051432\n",
            "EPOCH:18/100 - Training Loss: 319.1137363552194, Validation Loss: 332.58438851492747\n",
            "EPOCH:19/100 - Training Loss: 318.75212464981263, Validation Loss: 342.44618239629835\n",
            "EPOCH:20/100 - Training Loss: 317.95861579694724, Validation Loss: 335.97591320219493\n",
            "EPOCH:21/100 - Training Loss: 317.5162983168281, Validation Loss: 333.81808704194566\n",
            "EPOCH:22/100 - Training Loss: 315.25050201051846, Validation Loss: 334.3348794119699\n",
            "EPOCH:23/100 - Training Loss: 314.3292203552683, Validation Loss: 332.0873523530506\n",
            "EPOCH:24/100 - Training Loss: 314.6871678391049, Validation Loss: 335.79524812244233\n",
            "EPOCH:25/100 - Training Loss: 312.7043416972377, Validation Loss: 332.8335229782831\n",
            "EPOCH:26/100 - Training Loss: 312.3795578986193, Validation Loss: 334.14022492908293\n",
            "EPOCH:27/100 - Training Loss: 311.0844781552408, Validation Loss: 337.257620093936\n",
            "EPOCH:28/100 - Training Loss: 310.5954865521634, Validation Loss: 333.81104663667224\n",
            "EPOCH:29/100 - Training Loss: 310.32169624273985, Validation Loss: 336.0324936639695\n",
            "EPOCH:30/100 - Training Loss: 309.2285412080533, Validation Loss: 334.10717235746836\n",
            "EPOCH:31/100 - Training Loss: 308.0794095424024, Validation Loss: 334.35606689453124\n",
            "EPOCH:32/100 - Training Loss: 307.4970072379829, Validation Loss: 334.45730474562873\n",
            "EPOCH:33/100 - Training Loss: 305.7451187534378, Validation Loss: 338.42339186895464\n",
            "EPOCH:34/100 - Training Loss: 305.0338283347628, Validation Loss: 340.16397966657365\n",
            "EPOCH:35/100 - Training Loss: 304.21494659205325, Validation Loss: 335.75480303083145\n",
            "EPOCH:36/100 - Training Loss: 303.64873663053305, Validation Loss: 334.78933483305434\n",
            "EPOCH:37/100 - Training Loss: 302.868336606993, Validation Loss: 334.54088178362167\n",
            "EPOCH:38/100 - Training Loss: 300.3847353259248, Validation Loss: 343.2256712413969\n",
            "EPOCH:39/100 - Training Loss: 299.37786195158674, Validation Loss: 336.85757431756883\n",
            "EPOCH:40/100 - Training Loss: 299.75819728368793, Validation Loss: 336.35794939313615\n",
            "EPOCH:41/100 - Training Loss: 297.0587113774193, Validation Loss: 338.52062436058407\n",
            "EPOCH:42/100 - Training Loss: 297.7383680320867, Validation Loss: 338.14854707263765\n",
            "EPOCH:43/100 - Training Loss: 295.42029260166504, Validation Loss: 335.52233436221167\n",
            "EPOCH:44/100 - Training Loss: 292.95659180415845, Validation Loss: 338.3236669631231\n",
            "EPOCH:45/100 - Training Loss: 293.2473900005186, Validation Loss: 337.22637343633744\n",
            "EPOCH:46/100 - Training Loss: 291.67780589829766, Validation Loss: 339.40236220586866\n",
            "EPOCH:47/100 - Training Loss: 289.99795027848927, Validation Loss: 350.54515482584634\n",
            "EPOCH:48/100 - Training Loss: 290.12663216238093, Validation Loss: 339.73846406482517\n",
            "EPOCH:49/100 - Training Loss: 288.1477348309428, Validation Loss: 338.2219951811291\n",
            "EPOCH:50/100 - Training Loss: 287.67948855306764, Validation Loss: 337.8595129103888\n",
            "EPOCH:51/100 - Training Loss: 287.0106361953626, Validation Loss: 338.1078796386719\n",
            "EPOCH:52/100 - Training Loss: 285.8024143774356, Validation Loss: 339.3444844563802\n",
            "EPOCH:53/100 - Training Loss: 284.863558420987, Validation Loss: 339.6746903192429\n",
            "EPOCH:54/100 - Training Loss: 284.13221622366893, Validation Loss: 338.6928038097563\n",
            "EPOCH:55/100 - Training Loss: 284.15528915150355, Validation Loss: 337.8928203764416\n",
            "EPOCH:56/100 - Training Loss: 284.54373219929334, Validation Loss: 338.18085414341516\n",
            "EPOCH:57/100 - Training Loss: 282.8288498288976, Validation Loss: 344.19869428362165\n",
            "EPOCH:58/100 - Training Loss: 282.2630828821005, Validation Loss: 337.5088630312965\n",
            "EPOCH:59/100 - Training Loss: 281.42560548258854, Validation Loss: 341.070700945173\n",
            "EPOCH:60/100 - Training Loss: 279.3443507920586, Validation Loss: 340.4780495779855\n",
            "EPOCH:61/100 - Training Loss: 280.3923975869409, Validation Loss: 343.3098215738932\n",
            "EPOCH:62/100 - Training Loss: 279.25587414215744, Validation Loss: 339.11086818150113\n",
            "EPOCH:63/100 - Training Loss: 277.98211693593026, Validation Loss: 342.47439909435457\n",
            "EPOCH:64/100 - Training Loss: 278.01586455206314, Validation Loss: 340.82813662574404\n",
            "EPOCH:65/100 - Training Loss: 277.2178810319923, Validation Loss: 340.57086050851007\n",
            "EPOCH:66/100 - Training Loss: 276.65779479761966, Validation Loss: 339.91139163062684\n",
            "EPOCH:67/100 - Training Loss: 275.6804503666188, Validation Loss: 342.69130859375\n",
            "EPOCH:68/100 - Training Loss: 277.08136168413915, Validation Loss: 342.02813909621466\n",
            "EPOCH:69/100 - Training Loss: 273.9111526598509, Validation Loss: 339.59347272600445\n",
            "EPOCH:70/100 - Training Loss: 274.2575610395263, Validation Loss: 338.4399398077102\n",
            "EPOCH:71/100 - Training Loss: 273.2918234485999, Validation Loss: 341.2463094075521\n",
            "EPOCH:72/100 - Training Loss: 273.26692784202413, Validation Loss: 339.97504039946057\n",
            "EPOCH:73/100 - Training Loss: 273.2604170733249, Validation Loss: 340.26318010602677\n",
            "EPOCH:74/100 - Training Loss: 271.29538817849533, Validation Loss: 339.66629929315474\n",
            "EPOCH:75/100 - Training Loss: 271.0174880471605, Validation Loss: 340.33947129022505\n",
            "EPOCH:76/100 - Training Loss: 270.2673537953088, Validation Loss: 344.7179876418341\n",
            "EPOCH:77/100 - Training Loss: 270.42116311740193, Validation Loss: 340.4416730608259\n",
            "EPOCH:78/100 - Training Loss: 269.7203413205386, Validation Loss: 341.7120579310826\n",
            "EPOCH:79/100 - Training Loss: 269.59217231768696, Validation Loss: 343.25104137602307\n",
            "EPOCH:80/100 - Training Loss: 265.49916256925087, Validation Loss: 340.4790557861328\n",
            "EPOCH:81/100 - Training Loss: 265.8300389765555, Validation Loss: 341.02398899623324\n",
            "EPOCH:82/100 - Training Loss: 263.79026663331825, Validation Loss: 344.0403159005301\n",
            "EPOCH:83/100 - Training Loss: 264.99615432994176, Validation Loss: 341.65723411923364\n",
            "EPOCH:84/100 - Training Loss: 265.58033841663445, Validation Loss: 342.62040172758554\n",
            "EPOCH:85/100 - Training Loss: 262.7448452605837, Validation Loss: 344.51378479003904\n",
            "EPOCH:86/100 - Training Loss: 260.56034667167575, Validation Loss: 343.6329573858352\n",
            "EPOCH:87/100 - Training Loss: 260.6427700434209, Validation Loss: 345.47277003696985\n",
            "EPOCH:88/100 - Training Loss: 261.71827896227416, Validation Loss: 343.9582268124535\n",
            "EPOCH:89/100 - Training Loss: 258.60269008445283, Validation Loss: 342.788916742234\n",
            "EPOCH:90/100 - Training Loss: 259.34922701305305, Validation Loss: 344.4491593133836\n",
            "EPOCH:91/100 - Training Loss: 257.67046185040533, Validation Loss: 346.10636349632625\n",
            "EPOCH:92/100 - Training Loss: 257.3579307865698, Validation Loss: 344.0052400134859\n",
            "EPOCH:93/100 - Training Loss: 256.5667986084567, Validation Loss: 346.3608934674944\n",
            "EPOCH:94/100 - Training Loss: 255.55225206616385, Validation Loss: 346.0688765752883\n",
            "EPOCH:95/100 - Training Loss: 255.85220058322807, Validation Loss: 352.83985755557103\n",
            "EPOCH:96/100 - Training Loss: 252.87969557368385, Validation Loss: 349.2506878080822\n",
            "EPOCH:97/100 - Training Loss: 253.5898620678303, Validation Loss: 350.09474109468005\n",
            "EPOCH:98/100 - Training Loss: 254.02591199123773, Validation Loss: 348.4009781610398\n",
            "EPOCH:99/100 - Training Loss: 251.18482251952543, Validation Loss: 352.6624763125465\n",
            "EPOCH:100/100 - Training Loss: 250.63905343576945, Validation Loss: 348.282026163737\n",
            "Best epoch is epoch: 13\n",
            "Predictions saved\n",
            "\n",
            "\n",
            "Processing seed: 4/10\n",
            "\n",
            "\n",
            "EPOCH:1/100 - Training Loss: 412.3479252635437, Validation Loss: 325.26158142089844\n",
            "EPOCH:2/100 - Training Loss: 359.9941268593144, Validation Loss: 315.99880560012093\n",
            "EPOCH:3/100 - Training Loss: 352.1996470157742, Validation Loss: 308.1855015345982\n",
            "EPOCH:4/100 - Training Loss: 347.54073419320554, Validation Loss: 308.6462107340495\n",
            "EPOCH:5/100 - Training Loss: 345.02097861351433, Validation Loss: 305.96116550990513\n",
            "EPOCH:6/100 - Training Loss: 343.1526148029045, Validation Loss: 308.86021408807665\n",
            "EPOCH:7/100 - Training Loss: 341.81369149656456, Validation Loss: 304.81861296154204\n",
            "EPOCH:8/100 - Training Loss: 341.6705678061255, Validation Loss: 304.17887209937686\n",
            "EPOCH:9/100 - Training Loss: 338.17034740948736, Validation Loss: 304.6458849225725\n",
            "EPOCH:10/100 - Training Loss: 336.96305508488405, Validation Loss: 303.62777448381695\n",
            "EPOCH:11/100 - Training Loss: 335.3434992314523, Validation Loss: 303.37437424432665\n",
            "EPOCH:12/100 - Training Loss: 333.9518289133586, Validation Loss: 305.66923072451635\n",
            "EPOCH:13/100 - Training Loss: 332.82759968250065, Validation Loss: 307.0003474644252\n",
            "EPOCH:14/100 - Training Loss: 332.36604804355375, Validation Loss: 304.6380259195964\n",
            "EPOCH:15/100 - Training Loss: 330.8422425117584, Validation Loss: 308.93519635881694\n",
            "EPOCH:16/100 - Training Loss: 329.8957056669176, Validation Loss: 308.3813050769624\n",
            "EPOCH:17/100 - Training Loss: 328.90642487519114, Validation Loss: 305.2657589867001\n",
            "EPOCH:18/100 - Training Loss: 327.47356602343285, Validation Loss: 304.29486796061195\n",
            "EPOCH:19/100 - Training Loss: 326.95215765104086, Validation Loss: 304.23331080845423\n",
            "EPOCH:20/100 - Training Loss: 325.9878723945709, Validation Loss: 304.285257539295\n",
            "EPOCH:21/100 - Training Loss: 324.9646796542876, Validation Loss: 305.9302276611328\n",
            "EPOCH:22/100 - Training Loss: 324.3442861333952, Validation Loss: 305.7607449486142\n",
            "EPOCH:23/100 - Training Loss: 323.4895479844123, Validation Loss: 308.1131410144624\n",
            "EPOCH:24/100 - Training Loss: 322.8342743648265, Validation Loss: 307.0320796421596\n",
            "EPOCH:25/100 - Training Loss: 322.0852239422127, Validation Loss: 304.63741106305804\n",
            "EPOCH:26/100 - Training Loss: 320.90021440237405, Validation Loss: 305.3053211030506\n",
            "EPOCH:27/100 - Training Loss: 320.1862346494397, Validation Loss: 305.4895310174851\n",
            "EPOCH:28/100 - Training Loss: 319.9339951034946, Validation Loss: 310.57267223539804\n",
            "EPOCH:29/100 - Training Loss: 319.5358449712858, Validation Loss: 308.3350224086216\n",
            "EPOCH:30/100 - Training Loss: 317.77345250386895, Validation Loss: 306.0162266322545\n",
            "EPOCH:31/100 - Training Loss: 316.9470690451829, Validation Loss: 306.1663600376674\n",
            "EPOCH:32/100 - Training Loss: 315.83569762018243, Validation Loss: 308.4236067998977\n",
            "EPOCH:33/100 - Training Loss: 316.4662474493423, Validation Loss: 308.74376264299667\n",
            "EPOCH:34/100 - Training Loss: 314.8743504271587, Validation Loss: 306.8827401297433\n",
            "EPOCH:35/100 - Training Loss: 314.1709633146664, Validation Loss: 311.23382859002976\n",
            "EPOCH:36/100 - Training Loss: 312.19818359229333, Validation Loss: 309.49936029343377\n",
            "EPOCH:37/100 - Training Loss: 312.55818380205613, Validation Loss: 316.725687953404\n",
            "EPOCH:38/100 - Training Loss: 310.8799435199018, Validation Loss: 308.256841750372\n",
            "EPOCH:39/100 - Training Loss: 309.50941307049663, Validation Loss: 310.0719680059524\n",
            "EPOCH:40/100 - Training Loss: 308.67736441309523, Validation Loss: 307.3403410412016\n",
            "EPOCH:41/100 - Training Loss: 308.30693910002424, Validation Loss: 309.81927315848213\n",
            "EPOCH:42/100 - Training Loss: 307.03730712016613, Validation Loss: 307.42997581845236\n",
            "EPOCH:43/100 - Training Loss: 305.5050742210807, Validation Loss: 308.58375534784227\n",
            "EPOCH:44/100 - Training Loss: 303.5714686901302, Validation Loss: 310.22675461542036\n",
            "EPOCH:45/100 - Training Loss: 303.9911353718978, Validation Loss: 309.84301612490697\n",
            "EPOCH:46/100 - Training Loss: 303.37377093913733, Validation Loss: 309.5662882486979\n",
            "EPOCH:47/100 - Training Loss: 301.4659410535862, Validation Loss: 311.0479954310826\n",
            "EPOCH:48/100 - Training Loss: 299.9289362504545, Validation Loss: 311.798195539202\n",
            "EPOCH:49/100 - Training Loss: 300.9415826911289, Validation Loss: 308.6681445893787\n",
            "EPOCH:50/100 - Training Loss: 298.49453873736763, Validation Loss: 310.73572838192894\n",
            "EPOCH:51/100 - Training Loss: 298.26390592807235, Validation Loss: 312.89854808989026\n",
            "EPOCH:52/100 - Training Loss: 297.6961605463506, Validation Loss: 309.53650963192894\n",
            "EPOCH:53/100 - Training Loss: 297.1833198565572, Validation Loss: 310.0660071963356\n",
            "EPOCH:54/100 - Training Loss: 294.9957566909972, Validation Loss: 309.5254661923363\n",
            "EPOCH:55/100 - Training Loss: 294.61434932881724, Validation Loss: 314.31444149925596\n",
            "EPOCH:56/100 - Training Loss: 294.44879483607616, Validation Loss: 316.7484537760417\n",
            "EPOCH:57/100 - Training Loss: 292.51568801989134, Validation Loss: 317.45003473191036\n",
            "EPOCH:58/100 - Training Loss: 293.6782110391767, Validation Loss: 309.63945341564363\n",
            "EPOCH:59/100 - Training Loss: 293.0724578165496, Validation Loss: 313.1527137393043\n",
            "EPOCH:60/100 - Training Loss: 290.41491466148943, Validation Loss: 311.2729384649368\n",
            "EPOCH:61/100 - Training Loss: 290.11342385958943, Validation Loss: 310.286223929269\n",
            "EPOCH:62/100 - Training Loss: 291.212479256787, Validation Loss: 311.25437898181735\n",
            "EPOCH:63/100 - Training Loss: 289.8512794840592, Validation Loss: 310.28884945824035\n",
            "EPOCH:64/100 - Training Loss: 288.82052473919487, Validation Loss: 311.38990870884487\n",
            "EPOCH:65/100 - Training Loss: 286.68905126166516, Validation Loss: 315.1992383684431\n",
            "EPOCH:66/100 - Training Loss: 287.8420343694937, Validation Loss: 313.1867774600074\n",
            "EPOCH:67/100 - Training Loss: 287.2002909621646, Validation Loss: 311.11949826195126\n",
            "EPOCH:68/100 - Training Loss: 286.81142902601874, Validation Loss: 312.82895318894157\n",
            "EPOCH:69/100 - Training Loss: 287.67074759763295, Validation Loss: 311.97479320707777\n",
            "EPOCH:70/100 - Training Loss: 285.480418512531, Validation Loss: 314.8257392519996\n",
            "EPOCH:71/100 - Training Loss: 284.04186094006377, Validation Loss: 310.97634858630954\n",
            "EPOCH:72/100 - Training Loss: 283.97563801403544, Validation Loss: 311.1374787830171\n",
            "EPOCH:73/100 - Training Loss: 284.84951499993593, Validation Loss: 312.87051173618863\n",
            "EPOCH:74/100 - Training Loss: 284.6150227548968, Validation Loss: 313.10911138625374\n",
            "EPOCH:75/100 - Training Loss: 283.4263514334376, Validation Loss: 313.5917981828962\n",
            "EPOCH:76/100 - Training Loss: 281.41800707905844, Validation Loss: 316.43257765997026\n",
            "EPOCH:77/100 - Training Loss: 282.8047846609767, Validation Loss: 312.47920663016185\n",
            "EPOCH:78/100 - Training Loss: 281.28385189666386, Validation Loss: 311.781831577846\n",
            "EPOCH:79/100 - Training Loss: 282.41134527019784, Validation Loss: 312.83810642787387\n",
            "EPOCH:80/100 - Training Loss: 279.76559003952866, Validation Loss: 311.76112104143414\n",
            "EPOCH:81/100 - Training Loss: 280.8489369868094, Validation Loss: 313.1391584123884\n",
            "EPOCH:82/100 - Training Loss: 279.56123606970885, Validation Loss: 312.48562767392116\n",
            "EPOCH:83/100 - Training Loss: 277.4666383146955, Validation Loss: 317.25347013927643\n",
            "EPOCH:84/100 - Training Loss: 278.5305684164771, Validation Loss: 313.41089869907927\n",
            "EPOCH:85/100 - Training Loss: 276.03141464196983, Validation Loss: 314.6767085484096\n",
            "EPOCH:86/100 - Training Loss: 275.50245996079184, Validation Loss: 314.2222961425781\n",
            "EPOCH:87/100 - Training Loss: 276.949835911571, Validation Loss: 312.8745819091797\n",
            "EPOCH:88/100 - Training Loss: 274.62011438337885, Validation Loss: 313.52875540597097\n",
            "EPOCH:89/100 - Training Loss: 272.58152614402314, Validation Loss: 314.051758539109\n",
            "EPOCH:90/100 - Training Loss: 272.7448138689938, Validation Loss: 314.61237095424104\n",
            "EPOCH:91/100 - Training Loss: 271.0529224149936, Validation Loss: 314.7880440848214\n",
            "EPOCH:92/100 - Training Loss: 271.542711899787, Validation Loss: 318.6049067905971\n",
            "EPOCH:93/100 - Training Loss: 267.6951457822522, Validation Loss: 318.1789906819661\n",
            "EPOCH:94/100 - Training Loss: 266.63194322244647, Validation Loss: 314.2741497221447\n",
            "EPOCH:95/100 - Training Loss: 269.8008815610608, Validation Loss: 316.3858408610026\n",
            "EPOCH:96/100 - Training Loss: 265.9900878505661, Validation Loss: 320.60163952055433\n",
            "EPOCH:97/100 - Training Loss: 264.9365525894347, Validation Loss: 317.3353521437872\n",
            "EPOCH:98/100 - Training Loss: 264.2108421780898, Validation Loss: 316.5538200741722\n",
            "EPOCH:99/100 - Training Loss: 264.70995747787003, Validation Loss: 313.3749165852865\n",
            "EPOCH:100/100 - Training Loss: 261.77103478698, Validation Loss: 316.82528105236236\n",
            "Best epoch is epoch: 11\n",
            "Predictions saved\n",
            "\n",
            "\n",
            "Processing seed: 5/10\n",
            "\n",
            "\n",
            "EPOCH:1/100 - Training Loss: 403.5967303437663, Validation Loss: 367.34878816150484\n",
            "EPOCH:2/100 - Training Loss: 355.2592092800823, Validation Loss: 338.2154818580264\n",
            "EPOCH:3/100 - Training Loss: 347.11065466250324, Validation Loss: 338.0809517996652\n",
            "EPOCH:4/100 - Training Loss: 344.8623094217305, Validation Loss: 340.19086013067334\n",
            "EPOCH:5/100 - Training Loss: 340.7312064159457, Validation Loss: 344.9669744582403\n",
            "EPOCH:6/100 - Training Loss: 338.68322808531985, Validation Loss: 338.0539822533017\n",
            "EPOCH:7/100 - Training Loss: 336.3857473223192, Validation Loss: 341.95149957566036\n",
            "EPOCH:8/100 - Training Loss: 335.5050277892047, Validation Loss: 333.6794623965309\n",
            "EPOCH:9/100 - Training Loss: 332.77204884094385, Validation Loss: 333.6644416445778\n",
            "EPOCH:10/100 - Training Loss: 331.4829239583527, Validation Loss: 333.71723124186195\n",
            "EPOCH:11/100 - Training Loss: 331.9667339825687, Validation Loss: 338.48663301013767\n",
            "EPOCH:12/100 - Training Loss: 329.19178808389813, Validation Loss: 328.1839162190755\n",
            "EPOCH:13/100 - Training Loss: 328.2552074714606, Validation Loss: 335.98384719122026\n",
            "EPOCH:14/100 - Training Loss: 326.80288142748265, Validation Loss: 344.0639937627883\n",
            "EPOCH:15/100 - Training Loss: 325.6072421563269, Validation Loss: 334.64190078008744\n",
            "EPOCH:16/100 - Training Loss: 325.60897000478957, Validation Loss: 328.8675208682106\n",
            "EPOCH:17/100 - Training Loss: 323.5063667388406, Validation Loss: 330.6338992164249\n",
            "EPOCH:18/100 - Training Loss: 322.31038223303017, Validation Loss: 330.2588447207496\n",
            "EPOCH:19/100 - Training Loss: 322.41344045852986, Validation Loss: 330.63245253789995\n",
            "EPOCH:20/100 - Training Loss: 320.8942092494919, Validation Loss: 331.0819084530785\n",
            "EPOCH:21/100 - Training Loss: 320.19094123931376, Validation Loss: 330.3433124360584\n",
            "EPOCH:22/100 - Training Loss: 320.6736962948892, Validation Loss: 330.47977469308034\n",
            "EPOCH:23/100 - Training Loss: 318.59548327461914, Validation Loss: 333.95446137927826\n",
            "EPOCH:24/100 - Training Loss: 317.4824207824853, Validation Loss: 336.0603729248047\n",
            "EPOCH:25/100 - Training Loss: 317.400162653593, Validation Loss: 328.0165069580078\n",
            "EPOCH:26/100 - Training Loss: 316.1061887536242, Validation Loss: 330.39304795038134\n",
            "EPOCH:27/100 - Training Loss: 315.77593211171734, Validation Loss: 332.40708501906624\n",
            "EPOCH:28/100 - Training Loss: 314.5266555749716, Validation Loss: 329.45225873674667\n",
            "EPOCH:29/100 - Training Loss: 313.7713073511966, Validation Loss: 336.05947934105285\n",
            "EPOCH:30/100 - Training Loss: 313.8576171073822, Validation Loss: 335.3770539783296\n",
            "EPOCH:31/100 - Training Loss: 312.90576477779126, Validation Loss: 335.1171643938337\n",
            "EPOCH:32/100 - Training Loss: 311.62074898876836, Validation Loss: 332.00106462751114\n",
            "EPOCH:33/100 - Training Loss: 311.0767111038536, Validation Loss: 327.7625026157924\n",
            "EPOCH:34/100 - Training Loss: 310.4638102310654, Validation Loss: 330.73339494977677\n",
            "EPOCH:35/100 - Training Loss: 309.78826012076513, Validation Loss: 327.82596900576635\n",
            "EPOCH:36/100 - Training Loss: 308.1016812563511, Validation Loss: 328.67962631952196\n",
            "EPOCH:37/100 - Training Loss: 307.7821456435757, Validation Loss: 332.48809204101565\n",
            "EPOCH:38/100 - Training Loss: 306.80587945177905, Validation Loss: 330.64674377441406\n",
            "EPOCH:39/100 - Training Loss: 306.2310736025717, Validation Loss: 329.1602573939732\n",
            "EPOCH:40/100 - Training Loss: 305.0946034360899, Validation Loss: 329.49331955682663\n",
            "EPOCH:41/100 - Training Loss: 304.4313924966962, Validation Loss: 337.22489071800595\n",
            "EPOCH:42/100 - Training Loss: 305.01256603159027, Validation Loss: 333.5244825090681\n",
            "EPOCH:43/100 - Training Loss: 302.2916079864866, Validation Loss: 331.22803649902346\n",
            "EPOCH:44/100 - Training Loss: 302.26493398443324, Validation Loss: 331.04258873349147\n",
            "EPOCH:45/100 - Training Loss: 299.7940823138471, Validation Loss: 333.0961129324777\n",
            "EPOCH:46/100 - Training Loss: 300.31708076859434, Validation Loss: 330.9010484967913\n",
            "EPOCH:47/100 - Training Loss: 299.51276942141584, Validation Loss: 332.42049487885976\n",
            "EPOCH:48/100 - Training Loss: 298.4038599419423, Validation Loss: 330.61161426362537\n",
            "EPOCH:49/100 - Training Loss: 297.12047132614975, Validation Loss: 330.7872174944196\n",
            "EPOCH:50/100 - Training Loss: 297.3610712383698, Validation Loss: 334.13646007719495\n",
            "EPOCH:51/100 - Training Loss: 294.79268939398355, Validation Loss: 329.98817225864957\n",
            "EPOCH:52/100 - Training Loss: 294.27613828745547, Validation Loss: 331.38583286830357\n",
            "EPOCH:53/100 - Training Loss: 293.37881191135307, Validation Loss: 333.6671638125465\n",
            "EPOCH:54/100 - Training Loss: 293.4095319506661, Validation Loss: 330.01573137555806\n",
            "EPOCH:55/100 - Training Loss: 291.331241671396, Validation Loss: 332.4195078531901\n",
            "EPOCH:56/100 - Training Loss: 290.3894788063796, Validation Loss: 333.49708964029946\n",
            "EPOCH:57/100 - Training Loss: 289.77757376076784, Validation Loss: 330.7578669956752\n",
            "EPOCH:58/100 - Training Loss: 290.103657214909, Validation Loss: 341.1878694080171\n",
            "EPOCH:59/100 - Training Loss: 287.99454767777985, Validation Loss: 333.2467118036179\n",
            "EPOCH:60/100 - Training Loss: 287.8269452115516, Validation Loss: 331.934225609189\n",
            "EPOCH:61/100 - Training Loss: 286.3619553741237, Validation Loss: 333.3721915108817\n",
            "EPOCH:62/100 - Training Loss: 285.3444806192257, Validation Loss: 331.4455288841611\n",
            "EPOCH:63/100 - Training Loss: 284.5005456564819, Validation Loss: 331.9523996988932\n",
            "EPOCH:64/100 - Training Loss: 283.2177554656327, Validation Loss: 331.4713855561756\n",
            "EPOCH:65/100 - Training Loss: 281.95699091799787, Validation Loss: 332.754694039481\n",
            "EPOCH:66/100 - Training Loss: 283.58277128306094, Validation Loss: 331.9325953892299\n",
            "EPOCH:67/100 - Training Loss: 280.99160196673046, Validation Loss: 333.0209055582682\n",
            "EPOCH:68/100 - Training Loss: 281.17263506249765, Validation Loss: 331.13097287132626\n",
            "EPOCH:69/100 - Training Loss: 279.24151895381954, Validation Loss: 336.91698172433036\n",
            "EPOCH:70/100 - Training Loss: 280.58682766279344, Validation Loss: 332.69562159946986\n",
            "EPOCH:71/100 - Training Loss: 278.548586187613, Validation Loss: 332.9425345284598\n",
            "EPOCH:72/100 - Training Loss: 277.7519987192814, Validation Loss: 332.51116056896393\n",
            "EPOCH:73/100 - Training Loss: 276.72303611737163, Validation Loss: 334.0409242175874\n",
            "EPOCH:74/100 - Training Loss: 278.3915872869742, Validation Loss: 331.97077099028087\n",
            "EPOCH:75/100 - Training Loss: 275.0366828936666, Validation Loss: 335.6918875558036\n",
            "EPOCH:76/100 - Training Loss: 275.2837729806832, Validation Loss: 335.5913593110584\n",
            "EPOCH:77/100 - Training Loss: 274.34230764384483, Validation Loss: 333.5388926188151\n",
            "EPOCH:78/100 - Training Loss: 273.8120890870015, Validation Loss: 335.63360668364027\n",
            "EPOCH:79/100 - Training Loss: 272.5366752096463, Validation Loss: 334.6651368640718\n",
            "EPOCH:80/100 - Training Loss: 272.3685703687281, Validation Loss: 334.32241705031623\n",
            "EPOCH:81/100 - Training Loss: 271.85704466608087, Validation Loss: 332.51367826915924\n",
            "EPOCH:82/100 - Training Loss: 271.4836478221957, Validation Loss: 332.59617411295574\n",
            "EPOCH:83/100 - Training Loss: 271.1245926376743, Validation Loss: 336.47384498232884\n",
            "EPOCH:84/100 - Training Loss: 269.0424079621891, Validation Loss: 336.14405023484005\n",
            "EPOCH:85/100 - Training Loss: 270.64529779475174, Validation Loss: 332.9937446230934\n",
            "EPOCH:86/100 - Training Loss: 268.22500521129524, Validation Loss: 340.94351835704987\n",
            "EPOCH:87/100 - Training Loss: 268.01332891641766, Validation Loss: 333.14700811476933\n",
            "EPOCH:88/100 - Training Loss: 268.4831522393056, Validation Loss: 333.5182184128534\n",
            "EPOCH:89/100 - Training Loss: 266.622504550688, Validation Loss: 333.50259384881883\n",
            "EPOCH:90/100 - Training Loss: 268.6257817489151, Validation Loss: 335.29439842587425\n",
            "EPOCH:91/100 - Training Loss: 267.2150987793551, Validation Loss: 339.14303806849887\n",
            "EPOCH:92/100 - Training Loss: 267.24467280713355, Validation Loss: 336.5952713739304\n",
            "EPOCH:93/100 - Training Loss: 266.0082605949165, Validation Loss: 337.88882024855843\n",
            "EPOCH:94/100 - Training Loss: 266.4994681638294, Validation Loss: 335.39784138997396\n",
            "EPOCH:95/100 - Training Loss: 265.4809373295677, Validation Loss: 338.03546171642483\n",
            "EPOCH:96/100 - Training Loss: 263.4035782916449, Validation Loss: 337.6143979027158\n",
            "EPOCH:97/100 - Training Loss: 263.0952161911848, Validation Loss: 336.29399922688805\n",
            "EPOCH:98/100 - Training Loss: 261.30712681226345, Validation Loss: 338.4653160458519\n",
            "EPOCH:99/100 - Training Loss: 260.07579269955437, Validation Loss: 338.3952911376953\n",
            "EPOCH:100/100 - Training Loss: 259.98637650519396, Validation Loss: 335.6905059814453\n",
            "Best epoch is epoch: 33\n",
            "Predictions saved\n",
            "\n",
            "\n",
            "Processing seed: 6/10\n",
            "\n",
            "\n",
            "EPOCH:1/100 - Training Loss: 407.3278548575244, Validation Loss: 368.54224897112164\n",
            "EPOCH:2/100 - Training Loss: 350.2940652342002, Validation Loss: 353.80882321312316\n",
            "EPOCH:3/100 - Training Loss: 342.0543661914179, Validation Loss: 346.6905306861514\n",
            "EPOCH:4/100 - Training Loss: 339.64785577232345, Validation Loss: 345.16643894740514\n",
            "EPOCH:5/100 - Training Loss: 335.1489057153961, Validation Loss: 348.12674894787017\n",
            "EPOCH:6/100 - Training Loss: 333.9624546387884, Validation Loss: 343.2726295107887\n",
            "EPOCH:7/100 - Training Loss: 331.7289207294619, Validation Loss: 342.34420209612165\n",
            "EPOCH:8/100 - Training Loss: 330.08744160144596, Validation Loss: 343.83987877255396\n",
            "EPOCH:9/100 - Training Loss: 328.4842980505459, Validation Loss: 346.8651620047433\n",
            "EPOCH:10/100 - Training Loss: 326.75858479866264, Validation Loss: 342.78146289643786\n",
            "EPOCH:11/100 - Training Loss: 325.29377495303646, Validation Loss: 345.5781004406157\n",
            "EPOCH:12/100 - Training Loss: 324.42828948173434, Validation Loss: 344.06475408644906\n",
            "EPOCH:13/100 - Training Loss: 322.88373017595603, Validation Loss: 344.3564687093099\n",
            "EPOCH:14/100 - Training Loss: 322.16022435008483, Validation Loss: 341.54029599144343\n",
            "EPOCH:15/100 - Training Loss: 320.2676939679785, Validation Loss: 341.9377243768601\n",
            "EPOCH:16/100 - Training Loss: 320.16741134898473, Validation Loss: 343.63392217726937\n",
            "EPOCH:17/100 - Training Loss: 318.79297199112705, Validation Loss: 341.112121000744\n",
            "EPOCH:18/100 - Training Loss: 317.702321673919, Validation Loss: 344.49254426502046\n",
            "EPOCH:19/100 - Training Loss: 317.35297825342144, Validation Loss: 342.40101216634116\n",
            "EPOCH:20/100 - Training Loss: 316.0711580699839, Validation Loss: 343.92103053501677\n",
            "EPOCH:21/100 - Training Loss: 314.8417801959418, Validation Loss: 346.6371185302734\n",
            "EPOCH:22/100 - Training Loss: 314.2490094168943, Validation Loss: 343.1628705705915\n",
            "EPOCH:23/100 - Training Loss: 313.46029812396284, Validation Loss: 342.42225472586495\n",
            "EPOCH:24/100 - Training Loss: 313.3095351699429, Validation Loss: 343.0437311081659\n",
            "EPOCH:25/100 - Training Loss: 311.6500156375275, Validation Loss: 343.33722214471726\n",
            "EPOCH:26/100 - Training Loss: 311.4041646443005, Validation Loss: 344.72116001674107\n",
            "EPOCH:27/100 - Training Loss: 310.18337837892955, Validation Loss: 343.73763355073476\n",
            "EPOCH:28/100 - Training Loss: 310.47668996005183, Validation Loss: 351.7732776460193\n",
            "EPOCH:29/100 - Training Loss: 310.2234100123294, Validation Loss: 345.957711210705\n",
            "EPOCH:30/100 - Training Loss: 307.143058321641, Validation Loss: 343.9960104806083\n",
            "EPOCH:31/100 - Training Loss: 306.87300357499953, Validation Loss: 345.7787103562128\n",
            "EPOCH:32/100 - Training Loss: 306.2034389158991, Validation Loss: 344.8596836635045\n",
            "EPOCH:33/100 - Training Loss: 306.28236143344344, Validation Loss: 346.24717857724147\n",
            "EPOCH:34/100 - Training Loss: 304.4048465191607, Validation Loss: 346.77145051502043\n",
            "EPOCH:35/100 - Training Loss: 303.23439983650155, Validation Loss: 345.3304882231213\n",
            "EPOCH:36/100 - Training Loss: 303.5368440760064, Validation Loss: 345.61538885207403\n",
            "EPOCH:37/100 - Training Loss: 301.0290975638961, Validation Loss: 346.0420657203311\n",
            "EPOCH:38/100 - Training Loss: 300.1908490310705, Validation Loss: 347.4972387404669\n",
            "EPOCH:39/100 - Training Loss: 299.08392712722815, Validation Loss: 349.7814425513858\n",
            "EPOCH:40/100 - Training Loss: 298.1405143100493, Validation Loss: 347.2514399937221\n",
            "EPOCH:41/100 - Training Loss: 297.07948095644855, Validation Loss: 347.0708760579427\n",
            "EPOCH:42/100 - Training Loss: 296.5188474996562, Validation Loss: 350.6719029017857\n",
            "EPOCH:43/100 - Training Loss: 294.98751496016837, Validation Loss: 347.1638222830636\n",
            "EPOCH:44/100 - Training Loss: 293.51004720246306, Validation Loss: 349.78151942661833\n",
            "EPOCH:45/100 - Training Loss: 292.4590830472887, Validation Loss: 347.95144435337613\n",
            "EPOCH:46/100 - Training Loss: 292.0170482917731, Validation Loss: 348.01985793340776\n",
            "EPOCH:47/100 - Training Loss: 290.2172282362326, Validation Loss: 352.6550550188337\n",
            "EPOCH:48/100 - Training Loss: 289.4701607642709, Validation Loss: 349.83978765578496\n",
            "EPOCH:49/100 - Training Loss: 289.08515558720774, Validation Loss: 346.841069539388\n",
            "EPOCH:50/100 - Training Loss: 289.5611976496075, Validation Loss: 349.53377729143415\n",
            "EPOCH:51/100 - Training Loss: 288.9128249539396, Validation Loss: 348.30206589471726\n",
            "EPOCH:52/100 - Training Loss: 286.82570435152985, Validation Loss: 347.349320765904\n",
            "EPOCH:53/100 - Training Loss: 285.48121351729145, Validation Loss: 349.066793241955\n",
            "EPOCH:54/100 - Training Loss: 285.29962873800275, Validation Loss: 348.7140369233631\n",
            "EPOCH:55/100 - Training Loss: 284.9631432508227, Validation Loss: 348.4438502720424\n",
            "EPOCH:56/100 - Training Loss: 284.4072419305405, Validation Loss: 353.0380118233817\n",
            "EPOCH:57/100 - Training Loss: 283.7609085410762, Validation Loss: 349.8122359502883\n",
            "EPOCH:58/100 - Training Loss: 283.58782556574783, Validation Loss: 348.6366872151693\n",
            "EPOCH:59/100 - Training Loss: 281.37588907027873, Validation Loss: 357.9203062511626\n",
            "EPOCH:60/100 - Training Loss: 280.9258708362079, Validation Loss: 351.95966099330354\n",
            "EPOCH:61/100 - Training Loss: 280.7568702424625, Validation Loss: 350.18870515369235\n",
            "EPOCH:62/100 - Training Loss: 279.3229389054109, Validation Loss: 348.05943065824965\n",
            "EPOCH:63/100 - Training Loss: 280.1640128816227, Validation Loss: 348.99910438174294\n",
            "EPOCH:64/100 - Training Loss: 281.1667277443096, Validation Loss: 351.1465945289249\n",
            "EPOCH:65/100 - Training Loss: 278.1434621515024, Validation Loss: 351.0692475818452\n",
            "EPOCH:66/100 - Training Loss: 277.07277958034615, Validation Loss: 349.3272160121373\n",
            "EPOCH:67/100 - Training Loss: 276.4334769237582, Validation Loss: 349.65696411132814\n",
            "EPOCH:68/100 - Training Loss: 277.47382822696807, Validation Loss: 350.13545953659786\n",
            "EPOCH:69/100 - Training Loss: 276.41311900427917, Validation Loss: 351.0729772658575\n",
            "EPOCH:70/100 - Training Loss: 276.13452496221356, Validation Loss: 349.7627888997396\n",
            "EPOCH:71/100 - Training Loss: 274.2984285377375, Validation Loss: 359.85023338681174\n",
            "EPOCH:72/100 - Training Loss: 275.8199751678685, Validation Loss: 352.3792685372489\n",
            "EPOCH:73/100 - Training Loss: 273.19924955891537, Validation Loss: 348.804540289016\n",
            "EPOCH:74/100 - Training Loss: 274.40682775820636, Validation Loss: 351.37920939127605\n",
            "EPOCH:75/100 - Training Loss: 272.18774918440135, Validation Loss: 351.5781956263951\n",
            "EPOCH:76/100 - Training Loss: 271.2163652588473, Validation Loss: 352.75713820684524\n",
            "EPOCH:77/100 - Training Loss: 270.994889216093, Validation Loss: 350.89915902274\n",
            "EPOCH:78/100 - Training Loss: 269.09200439817295, Validation Loss: 357.8416778564453\n",
            "EPOCH:79/100 - Training Loss: 270.06430106515813, Validation Loss: 353.07598978678385\n",
            "EPOCH:80/100 - Training Loss: 269.1143160617437, Validation Loss: 353.9380882626488\n",
            "EPOCH:81/100 - Training Loss: 269.2590025216697, Validation Loss: 353.22861531575523\n",
            "EPOCH:82/100 - Training Loss: 267.8623173424623, Validation Loss: 356.50145336332776\n",
            "EPOCH:83/100 - Training Loss: 266.33607431926134, Validation Loss: 350.02357105073474\n",
            "EPOCH:84/100 - Training Loss: 265.69429402135154, Validation Loss: 352.951413835798\n",
            "EPOCH:85/100 - Training Loss: 264.2449326253449, Validation Loss: 351.78353038969493\n",
            "EPOCH:86/100 - Training Loss: 263.61952162130353, Validation Loss: 356.3916909354074\n",
            "EPOCH:87/100 - Training Loss: 263.50156568285956, Validation Loss: 354.0141649518694\n",
            "EPOCH:88/100 - Training Loss: 263.36124815292175, Validation Loss: 355.2290055047898\n",
            "EPOCH:89/100 - Training Loss: 260.57608571200495, Validation Loss: 362.917919195266\n",
            "EPOCH:90/100 - Training Loss: 260.46023246382754, Validation Loss: 352.91250392368863\n",
            "EPOCH:91/100 - Training Loss: 261.0899355576545, Validation Loss: 356.15549127487907\n",
            "EPOCH:92/100 - Training Loss: 261.15095621987575, Validation Loss: 355.9136012486049\n",
            "EPOCH:93/100 - Training Loss: 258.53497824293333, Validation Loss: 354.2020022437686\n",
            "EPOCH:94/100 - Training Loss: 259.81363588756477, Validation Loss: 355.69147149948844\n",
            "EPOCH:95/100 - Training Loss: 255.8717635525724, Validation Loss: 353.84595889136904\n",
            "EPOCH:96/100 - Training Loss: 255.2287639900153, Validation Loss: 351.5136072067987\n",
            "EPOCH:97/100 - Training Loss: 255.0904362935724, Validation Loss: 358.5236814953032\n",
            "EPOCH:98/100 - Training Loss: 256.3683494440411, Validation Loss: 354.1416463216146\n",
            "EPOCH:99/100 - Training Loss: 255.6984219607988, Validation Loss: 353.97107660202755\n",
            "EPOCH:100/100 - Training Loss: 253.90867042996717, Validation Loss: 353.36034604027157\n",
            "Best epoch is epoch: 17\n",
            "Predictions saved\n",
            "\n",
            "\n",
            "Processing seed: 7/10\n",
            "\n",
            "\n",
            "EPOCH:1/100 - Training Loss: 412.75424507523496, Validation Loss: 335.03109770275296\n",
            "EPOCH:2/100 - Training Loss: 359.473794723183, Validation Loss: 311.9524111793155\n",
            "EPOCH:3/100 - Training Loss: 352.9256619109743, Validation Loss: 308.0905308314732\n",
            "EPOCH:4/100 - Training Loss: 348.96644832638395, Validation Loss: 310.7775522867839\n",
            "EPOCH:5/100 - Training Loss: 345.14277619792057, Validation Loss: 305.7306960332961\n",
            "EPOCH:6/100 - Training Loss: 342.43582641310223, Validation Loss: 316.81519484747025\n",
            "EPOCH:7/100 - Training Loss: 342.47599355474574, Validation Loss: 307.6142329624721\n",
            "EPOCH:8/100 - Training Loss: 339.8888695473318, Validation Loss: 306.1221826462519\n",
            "EPOCH:9/100 - Training Loss: 338.40017834938794, Validation Loss: 307.30756138392854\n",
            "EPOCH:10/100 - Training Loss: 336.5763684548171, Validation Loss: 306.03242056710377\n",
            "EPOCH:11/100 - Training Loss: 335.0157372740971, Validation Loss: 309.35294567289804\n",
            "EPOCH:12/100 - Training Loss: 334.59841339912504, Validation Loss: 304.7982087634859\n",
            "EPOCH:13/100 - Training Loss: 333.13706369331743, Validation Loss: 306.4329922630673\n",
            "EPOCH:14/100 - Training Loss: 331.90441246305846, Validation Loss: 309.38043823242185\n",
            "EPOCH:15/100 - Training Loss: 330.898365430445, Validation Loss: 305.79809221540177\n",
            "EPOCH:16/100 - Training Loss: 330.2360846398838, Validation Loss: 309.38898184640067\n",
            "EPOCH:17/100 - Training Loss: 328.6990279240938, Validation Loss: 304.1408364432199\n",
            "EPOCH:18/100 - Training Loss: 327.7536761299807, Validation Loss: 305.1118693033854\n",
            "EPOCH:19/100 - Training Loss: 326.68236448884295, Validation Loss: 306.5786204020182\n",
            "EPOCH:20/100 - Training Loss: 326.13852464043157, Validation Loss: 308.42062000093006\n",
            "EPOCH:21/100 - Training Loss: 324.49793959005353, Validation Loss: 307.16016540527346\n",
            "EPOCH:22/100 - Training Loss: 323.967606209912, Validation Loss: 305.28050900413876\n",
            "EPOCH:23/100 - Training Loss: 324.3440682131237, Validation Loss: 305.4262906029111\n",
            "EPOCH:24/100 - Training Loss: 322.13835388037927, Validation Loss: 306.5595418294271\n",
            "EPOCH:25/100 - Training Loss: 321.76497156694, Validation Loss: 306.84569178989955\n",
            "EPOCH:26/100 - Training Loss: 321.42482926908144, Validation Loss: 308.6837637765067\n",
            "EPOCH:27/100 - Training Loss: 319.60852728140384, Validation Loss: 307.91522289457777\n",
            "EPOCH:28/100 - Training Loss: 319.35905074162815, Validation Loss: 305.772505405971\n",
            "EPOCH:29/100 - Training Loss: 318.34177429636225, Validation Loss: 311.80130891345794\n",
            "EPOCH:30/100 - Training Loss: 317.8799666447969, Validation Loss: 312.08934079124816\n",
            "EPOCH:31/100 - Training Loss: 317.3084416355324, Validation Loss: 306.44744378952754\n",
            "EPOCH:32/100 - Training Loss: 316.6063730608592, Validation Loss: 306.1911087762742\n",
            "EPOCH:33/100 - Training Loss: 314.7832175097773, Validation Loss: 306.5066183907645\n",
            "EPOCH:34/100 - Training Loss: 314.0805612350136, Validation Loss: 306.4208090645926\n",
            "EPOCH:35/100 - Training Loss: 313.2952958427921, Validation Loss: 307.6175541469029\n",
            "EPOCH:36/100 - Training Loss: 312.16474637780385, Validation Loss: 307.69389096214655\n",
            "EPOCH:37/100 - Training Loss: 311.18150620927105, Validation Loss: 316.34798395066036\n",
            "EPOCH:38/100 - Training Loss: 311.2097123539818, Validation Loss: 317.7732956659226\n",
            "EPOCH:39/100 - Training Loss: 311.38143534876565, Validation Loss: 307.5277102515811\n",
            "EPOCH:40/100 - Training Loss: 309.1507014454407, Validation Loss: 312.6768006824312\n",
            "EPOCH:41/100 - Training Loss: 307.77346404810794, Validation Loss: 310.48906976609004\n",
            "EPOCH:42/100 - Training Loss: 307.78622627713514, Validation Loss: 307.650483921596\n",
            "EPOCH:43/100 - Training Loss: 306.49669288507795, Validation Loss: 312.34956999279206\n",
            "EPOCH:44/100 - Training Loss: 305.0414922891767, Validation Loss: 308.6592837379092\n",
            "EPOCH:45/100 - Training Loss: 303.7442993673903, Validation Loss: 315.8740939185733\n",
            "EPOCH:46/100 - Training Loss: 302.96738645057405, Validation Loss: 311.1984356108166\n",
            "EPOCH:47/100 - Training Loss: 301.41224200628824, Validation Loss: 308.98928803943454\n",
            "EPOCH:48/100 - Training Loss: 300.8454899025419, Validation Loss: 311.0945324125744\n",
            "EPOCH:49/100 - Training Loss: 301.74243331581425, Validation Loss: 310.1407719203404\n",
            "EPOCH:50/100 - Training Loss: 299.6751775991945, Validation Loss: 308.9976935977028\n",
            "EPOCH:51/100 - Training Loss: 298.6598462853625, Validation Loss: 310.39936102004276\n",
            "EPOCH:52/100 - Training Loss: 296.4909643569254, Validation Loss: 309.93556823730466\n",
            "EPOCH:53/100 - Training Loss: 295.12004074788604, Validation Loss: 311.83373166038876\n",
            "EPOCH:54/100 - Training Loss: 295.4605419368334, Validation Loss: 309.6114513578869\n",
            "EPOCH:55/100 - Training Loss: 293.6616989172159, Validation Loss: 314.2326171875\n",
            "EPOCH:56/100 - Training Loss: 293.94714169741246, Validation Loss: 313.3219265892392\n",
            "EPOCH:57/100 - Training Loss: 291.2080162976977, Validation Loss: 315.571582757859\n",
            "EPOCH:58/100 - Training Loss: 291.7385555986573, Validation Loss: 309.81952500116256\n",
            "EPOCH:59/100 - Training Loss: 292.5028778112589, Validation Loss: 309.71998014904204\n",
            "EPOCH:60/100 - Training Loss: 288.3984214218249, Validation Loss: 312.1424668085007\n",
            "EPOCH:61/100 - Training Loss: 289.2119894642249, Validation Loss: 312.3299822126116\n",
            "EPOCH:62/100 - Training Loss: 289.3153905208469, Validation Loss: 309.6815223330543\n",
            "EPOCH:63/100 - Training Loss: 287.29077556309664, Validation Loss: 311.19319893973216\n",
            "EPOCH:64/100 - Training Loss: 286.4743266868136, Validation Loss: 310.8486538841611\n",
            "EPOCH:65/100 - Training Loss: 285.79144716831837, Validation Loss: 309.6855070568266\n",
            "EPOCH:66/100 - Training Loss: 285.51792000927617, Validation Loss: 310.5439201718285\n",
            "EPOCH:67/100 - Training Loss: 286.07981095052276, Validation Loss: 309.64953366234187\n",
            "EPOCH:68/100 - Training Loss: 283.7378107949487, Validation Loss: 311.84662054152716\n",
            "EPOCH:69/100 - Training Loss: 285.3285765909637, Validation Loss: 311.4695074172247\n",
            "EPOCH:70/100 - Training Loss: 282.93652664220986, Validation Loss: 313.4772224062965\n",
            "EPOCH:71/100 - Training Loss: 282.62620482524426, Validation Loss: 313.43211669921874\n",
            "EPOCH:72/100 - Training Loss: 282.83850554691577, Validation Loss: 318.0930136544364\n",
            "EPOCH:73/100 - Training Loss: 280.9897759376107, Validation Loss: 314.8364503406343\n",
            "EPOCH:74/100 - Training Loss: 279.8498014937153, Validation Loss: 313.1233125232515\n",
            "EPOCH:75/100 - Training Loss: 279.9742892681841, Validation Loss: 316.6979445684524\n",
            "EPOCH:76/100 - Training Loss: 278.6597062613913, Validation Loss: 315.10525280180434\n",
            "EPOCH:77/100 - Training Loss: 277.7262440865819, Validation Loss: 312.7861762637184\n",
            "EPOCH:78/100 - Training Loss: 278.4278426067926, Validation Loss: 314.17050388881137\n",
            "EPOCH:79/100 - Training Loss: 277.15583072926376, Validation Loss: 313.1236373174758\n",
            "EPOCH:80/100 - Training Loss: 276.2603992835434, Validation Loss: 312.2591600690569\n",
            "EPOCH:81/100 - Training Loss: 274.66234290002353, Validation Loss: 314.60843781970794\n",
            "EPOCH:82/100 - Training Loss: 275.71979273247547, Validation Loss: 313.3162067231678\n",
            "EPOCH:83/100 - Training Loss: 273.5261650176492, Validation Loss: 315.5194638206845\n",
            "EPOCH:84/100 - Training Loss: 274.0617522647102, Validation Loss: 314.69993169875374\n",
            "EPOCH:85/100 - Training Loss: 272.9225228794435, Validation Loss: 321.8259447370257\n",
            "EPOCH:86/100 - Training Loss: 273.7720598207169, Validation Loss: 315.1355287097749\n",
            "EPOCH:87/100 - Training Loss: 271.23121677662704, Validation Loss: 315.0561067127046\n",
            "EPOCH:88/100 - Training Loss: 269.63649691488405, Validation Loss: 317.14694126674107\n",
            "EPOCH:89/100 - Training Loss: 269.84585689644825, Validation Loss: 314.71433890206475\n",
            "EPOCH:90/100 - Training Loss: 270.05800510306346, Validation Loss: 322.84962550571987\n",
            "EPOCH:91/100 - Training Loss: 270.4451510627401, Validation Loss: 319.46097557431176\n",
            "EPOCH:92/100 - Training Loss: 268.14735982526173, Validation Loss: 320.19601004464283\n",
            "EPOCH:93/100 - Training Loss: 268.9583411569527, Validation Loss: 314.44291018531436\n",
            "EPOCH:94/100 - Training Loss: 266.746831707283, Validation Loss: 317.7551061721075\n",
            "EPOCH:95/100 - Training Loss: 265.87754022420734, Validation Loss: 316.58421747116813\n",
            "EPOCH:96/100 - Training Loss: 264.675479952646, Validation Loss: 323.02354838053384\n",
            "EPOCH:97/100 - Training Loss: 265.4528461902409, Validation Loss: 314.7056146530878\n",
            "EPOCH:98/100 - Training Loss: 265.197473275633, Validation Loss: 315.14107680547806\n",
            "EPOCH:99/100 - Training Loss: 265.2960329260633, Validation Loss: 316.2550483340309\n",
            "EPOCH:100/100 - Training Loss: 262.2090021647815, Validation Loss: 313.6341336204892\n",
            "Best epoch is epoch: 17\n",
            "Predictions saved\n",
            "\n",
            "\n",
            "Processing seed: 8/10\n",
            "\n",
            "\n",
            "EPOCH:1/100 - Training Loss: 406.5665715110615, Validation Loss: 361.8569426037016\n",
            "EPOCH:2/100 - Training Loss: 351.0645213343363, Validation Loss: 348.45355035691034\n",
            "EPOCH:3/100 - Training Loss: 344.031900883859, Validation Loss: 358.0770105271112\n",
            "EPOCH:4/100 - Training Loss: 339.10989470925705, Validation Loss: 342.03724481491815\n",
            "EPOCH:5/100 - Training Loss: 336.74110055709514, Validation Loss: 343.35929231189544\n",
            "EPOCH:6/100 - Training Loss: 335.1700307987186, Validation Loss: 340.49166172572546\n",
            "EPOCH:7/100 - Training Loss: 332.81164186609675, Validation Loss: 340.7116549537295\n",
            "EPOCH:8/100 - Training Loss: 331.9380245163219, Validation Loss: 343.1058143252418\n",
            "EPOCH:9/100 - Training Loss: 330.55782276235504, Validation Loss: 344.1297623407273\n",
            "EPOCH:10/100 - Training Loss: 327.80634859581266, Validation Loss: 348.2256674630301\n",
            "EPOCH:11/100 - Training Loss: 327.68089156048393, Validation Loss: 339.7947022937593\n",
            "EPOCH:12/100 - Training Loss: 326.4683115010045, Validation Loss: 338.3032749720982\n",
            "EPOCH:13/100 - Training Loss: 324.3268191899775, Validation Loss: 344.4886224655878\n",
            "EPOCH:14/100 - Training Loss: 323.9200140832432, Validation Loss: 345.9404632568359\n",
            "EPOCH:15/100 - Training Loss: 322.66748035949854, Validation Loss: 341.6417375837054\n",
            "EPOCH:16/100 - Training Loss: 322.2251200892191, Validation Loss: 338.51867138090586\n",
            "EPOCH:17/100 - Training Loss: 320.87076992863405, Validation Loss: 338.61751229422435\n",
            "EPOCH:18/100 - Training Loss: 320.22673588208767, Validation Loss: 340.9373350597563\n",
            "EPOCH:19/100 - Training Loss: 318.99894215440406, Validation Loss: 340.25572974795386\n",
            "EPOCH:20/100 - Training Loss: 317.91215191024014, Validation Loss: 339.57865745907736\n",
            "EPOCH:21/100 - Training Loss: 317.46061313237664, Validation Loss: 339.72795642671133\n",
            "EPOCH:22/100 - Training Loss: 316.8771719192833, Validation Loss: 341.4412631080264\n",
            "EPOCH:23/100 - Training Loss: 316.1969930425749, Validation Loss: 339.97638215564547\n",
            "EPOCH:24/100 - Training Loss: 315.37923258414986, Validation Loss: 340.40130687895277\n",
            "EPOCH:25/100 - Training Loss: 315.0365641373154, Validation Loss: 343.64212777273997\n",
            "EPOCH:26/100 - Training Loss: 312.8789470007994, Validation Loss: 349.4672584170387\n",
            "EPOCH:27/100 - Training Loss: 311.81712385009183, Validation Loss: 343.89803830101374\n",
            "EPOCH:28/100 - Training Loss: 311.24944179780726, Validation Loss: 341.1994283040365\n",
            "EPOCH:29/100 - Training Loss: 311.911202332854, Validation Loss: 343.41536894298736\n",
            "EPOCH:30/100 - Training Loss: 309.7133984986808, Validation Loss: 343.7340039934431\n",
            "EPOCH:31/100 - Training Loss: 309.26867825091597, Validation Loss: 350.21480945405506\n",
            "EPOCH:32/100 - Training Loss: 308.89906194500253, Validation Loss: 342.3673066638765\n",
            "EPOCH:33/100 - Training Loss: 307.2583533312085, Validation Loss: 343.3056056431362\n",
            "EPOCH:34/100 - Training Loss: 307.2011683425357, Validation Loss: 341.33147771926156\n",
            "EPOCH:35/100 - Training Loss: 306.15540734338873, Validation Loss: 344.3547845749628\n",
            "EPOCH:36/100 - Training Loss: 305.59181031293116, Validation Loss: 343.76656930106026\n",
            "EPOCH:37/100 - Training Loss: 304.07878797435535, Validation Loss: 342.6577405657087\n",
            "EPOCH:38/100 - Training Loss: 303.38060157794087, Validation Loss: 341.43710138230097\n",
            "EPOCH:39/100 - Training Loss: 302.23688366874023, Validation Loss: 342.180230422247\n",
            "EPOCH:40/100 - Training Loss: 302.11335080677117, Validation Loss: 347.1245743524461\n",
            "EPOCH:41/100 - Training Loss: 301.3271437761038, Validation Loss: 342.4018508184524\n",
            "EPOCH:42/100 - Training Loss: 298.82206740641084, Validation Loss: 342.74654976981026\n",
            "EPOCH:43/100 - Training Loss: 299.2285629490964, Validation Loss: 342.71685180664065\n",
            "EPOCH:44/100 - Training Loss: 297.18206342820054, Validation Loss: 342.453700764974\n",
            "EPOCH:45/100 - Training Loss: 296.6884188230965, Validation Loss: 344.069434320359\n",
            "EPOCH:46/100 - Training Loss: 297.59212768846027, Validation Loss: 343.29385666620163\n",
            "EPOCH:47/100 - Training Loss: 294.6180492938276, Validation Loss: 345.01156935918897\n",
            "EPOCH:48/100 - Training Loss: 295.10134833069577, Validation Loss: 345.1201102120536\n",
            "EPOCH:49/100 - Training Loss: 294.7088900909788, Validation Loss: 344.6068302699498\n",
            "EPOCH:50/100 - Training Loss: 291.74288760164757, Validation Loss: 343.60291166759674\n",
            "EPOCH:51/100 - Training Loss: 292.36599616731263, Validation Loss: 344.0828667050316\n",
            "EPOCH:52/100 - Training Loss: 290.5095577922812, Validation Loss: 345.32938377743676\n",
            "EPOCH:53/100 - Training Loss: 289.67113806239746, Validation Loss: 343.8535544259208\n",
            "EPOCH:54/100 - Training Loss: 289.21028149953037, Validation Loss: 368.4191632952009\n",
            "EPOCH:55/100 - Training Loss: 288.17054278753824, Validation Loss: 345.0307088216146\n",
            "EPOCH:56/100 - Training Loss: 287.398871738188, Validation Loss: 344.8509272984096\n",
            "EPOCH:57/100 - Training Loss: 286.16468718171404, Validation Loss: 348.3825185139974\n",
            "EPOCH:58/100 - Training Loss: 285.59416592035774, Validation Loss: 344.5517139253162\n",
            "EPOCH:59/100 - Training Loss: 285.20817516754806, Validation Loss: 346.8973629906064\n",
            "EPOCH:60/100 - Training Loss: 285.13585140255583, Validation Loss: 351.76988220214844\n",
            "EPOCH:61/100 - Training Loss: 283.45026038996076, Validation Loss: 355.71367986769906\n",
            "EPOCH:62/100 - Training Loss: 282.96115423671387, Validation Loss: 345.2264918736049\n",
            "EPOCH:63/100 - Training Loss: 280.185859279587, Validation Loss: 343.9302286783854\n",
            "EPOCH:64/100 - Training Loss: 282.01557747560923, Validation Loss: 345.48964015415737\n",
            "EPOCH:65/100 - Training Loss: 279.5775797987326, Validation Loss: 344.7131607782273\n",
            "EPOCH:66/100 - Training Loss: 279.28053057905026, Validation Loss: 345.77648475283667\n",
            "EPOCH:67/100 - Training Loss: 278.3375582273934, Validation Loss: 348.92743094308037\n",
            "EPOCH:68/100 - Training Loss: 278.56589477102057, Validation Loss: 344.6940861293248\n",
            "EPOCH:69/100 - Training Loss: 280.29579630558135, Validation Loss: 352.27765677315847\n",
            "EPOCH:70/100 - Training Loss: 277.57891108255683, Validation Loss: 345.6964477539062\n",
            "EPOCH:71/100 - Training Loss: 277.91912559563906, Validation Loss: 345.55736941383\n",
            "EPOCH:72/100 - Training Loss: 276.8944961620686, Validation Loss: 357.3750665573847\n",
            "EPOCH:73/100 - Training Loss: 276.29176366983563, Validation Loss: 348.4671642485119\n",
            "EPOCH:74/100 - Training Loss: 275.17771403601745, Validation Loss: 347.53779064360117\n",
            "EPOCH:75/100 - Training Loss: 274.92026681831743, Validation Loss: 348.44728306361606\n",
            "EPOCH:76/100 - Training Loss: 275.543572164094, Validation Loss: 348.5909563337054\n",
            "EPOCH:77/100 - Training Loss: 274.24292108722403, Validation Loss: 346.38092113676527\n",
            "EPOCH:78/100 - Training Loss: 274.5766431130202, Validation Loss: 350.4840824672154\n",
            "EPOCH:79/100 - Training Loss: 273.652378164214, Validation Loss: 347.7969494047619\n",
            "EPOCH:80/100 - Training Loss: 273.3875483692688, Validation Loss: 353.37102370489214\n",
            "EPOCH:81/100 - Training Loss: 271.6839864726283, Validation Loss: 351.8106199718657\n",
            "EPOCH:82/100 - Training Loss: 272.016317818216, Validation Loss: 347.16498602004276\n",
            "EPOCH:83/100 - Training Loss: 272.3947100946613, Validation Loss: 347.0446544828869\n",
            "EPOCH:84/100 - Training Loss: 270.7096614610039, Validation Loss: 345.97578866141185\n",
            "EPOCH:85/100 - Training Loss: 270.8225502068786, Validation Loss: 346.02540443057103\n",
            "EPOCH:86/100 - Training Loss: 271.45241186101, Validation Loss: 354.70493977864584\n",
            "EPOCH:87/100 - Training Loss: 270.7106497680373, Validation Loss: 351.7963367280506\n",
            "EPOCH:88/100 - Training Loss: 269.77821084252406, Validation Loss: 346.64627947126115\n",
            "EPOCH:89/100 - Training Loss: 268.53953672422716, Validation Loss: 347.90290992373514\n",
            "EPOCH:90/100 - Training Loss: 269.5134887331141, Validation Loss: 349.0815912155878\n",
            "EPOCH:91/100 - Training Loss: 268.02045762965673, Validation Loss: 354.6625281924293\n",
            "EPOCH:92/100 - Training Loss: 267.72446317832055, Validation Loss: 347.5783422560919\n",
            "EPOCH:93/100 - Training Loss: 269.8849894994768, Validation Loss: 350.2038226899647\n",
            "EPOCH:94/100 - Training Loss: 267.3887092754209, Validation Loss: 348.81258719308033\n",
            "EPOCH:95/100 - Training Loss: 266.8042210326274, Validation Loss: 347.05891898018973\n",
            "EPOCH:96/100 - Training Loss: 264.48522377469374, Validation Loss: 351.3158982049851\n",
            "EPOCH:97/100 - Training Loss: 263.53033274284127, Validation Loss: 346.4557397751581\n",
            "EPOCH:98/100 - Training Loss: 263.03546275500753, Validation Loss: 356.2116459437779\n",
            "EPOCH:99/100 - Training Loss: 260.2527720012073, Validation Loss: 347.63648274739586\n",
            "EPOCH:100/100 - Training Loss: 260.81239766778697, Validation Loss: 351.8211701892671\n",
            "Best epoch is epoch: 12\n",
            "Predictions saved\n",
            "\n",
            "\n",
            "Processing seed: 9/10\n",
            "\n",
            "\n",
            "EPOCH:1/100 - Training Loss: 408.41603044678317, Validation Loss: 354.26485232398625\n",
            "EPOCH:2/100 - Training Loss: 355.43970582365705, Validation Loss: 348.820362781343\n",
            "EPOCH:3/100 - Training Loss: 349.4392151752918, Validation Loss: 327.4089903331938\n",
            "EPOCH:4/100 - Training Loss: 345.7641065866111, Validation Loss: 331.02718171619233\n",
            "EPOCH:5/100 - Training Loss: 341.9896364781054, Validation Loss: 328.00965198335194\n",
            "EPOCH:6/100 - Training Loss: 341.1693469573319, Validation Loss: 324.7539074125744\n",
            "EPOCH:7/100 - Training Loss: 338.67130794843797, Validation Loss: 324.2188669840495\n",
            "EPOCH:8/100 - Training Loss: 337.2240736057764, Validation Loss: 324.56004798525856\n",
            "EPOCH:9/100 - Training Loss: 333.96330781995823, Validation Loss: 328.3796603248233\n",
            "EPOCH:10/100 - Training Loss: 333.91784431257224, Validation Loss: 331.924701218378\n",
            "EPOCH:11/100 - Training Loss: 332.14080679445107, Validation Loss: 331.829046921503\n",
            "EPOCH:12/100 - Training Loss: 330.4858722914375, Validation Loss: 325.54514072963167\n",
            "EPOCH:13/100 - Training Loss: 330.66405791143814, Validation Loss: 324.0624228341239\n",
            "EPOCH:14/100 - Training Loss: 328.10294383920746, Validation Loss: 325.8907677060082\n",
            "EPOCH:15/100 - Training Loss: 327.79290214301864, Validation Loss: 328.414984421503\n",
            "EPOCH:16/100 - Training Loss: 326.65359264000506, Validation Loss: 331.8202396937779\n",
            "EPOCH:17/100 - Training Loss: 325.5432046603474, Validation Loss: 331.3375433058966\n",
            "EPOCH:18/100 - Training Loss: 324.5436689426905, Validation Loss: 324.0391002836682\n",
            "EPOCH:19/100 - Training Loss: 324.28618233983445, Validation Loss: 326.4445604596819\n",
            "EPOCH:20/100 - Training Loss: 323.123439306291, Validation Loss: 325.11748686290923\n",
            "EPOCH:21/100 - Training Loss: 321.57223794796016, Validation Loss: 324.81663309733074\n",
            "EPOCH:22/100 - Training Loss: 321.627732197254, Validation Loss: 324.64530610584075\n",
            "EPOCH:23/100 - Training Loss: 319.71131875122364, Validation Loss: 325.007318405878\n",
            "EPOCH:24/100 - Training Loss: 320.06141218308335, Validation Loss: 324.57069614955356\n",
            "EPOCH:25/100 - Training Loss: 318.9231168614936, Validation Loss: 325.48517426990327\n",
            "EPOCH:26/100 - Training Loss: 318.3754176319641, Validation Loss: 325.2214137486049\n",
            "EPOCH:27/100 - Training Loss: 316.97233264713697, Validation Loss: 325.96288030715215\n",
            "EPOCH:28/100 - Training Loss: 317.36720068301105, Validation Loss: 334.5094787597656\n",
            "EPOCH:29/100 - Training Loss: 315.68417551409374, Validation Loss: 326.2328972226097\n",
            "EPOCH:30/100 - Training Loss: 315.11978448034756, Validation Loss: 326.48613804408484\n",
            "EPOCH:31/100 - Training Loss: 314.6751857566378, Validation Loss: 337.9973770868211\n",
            "EPOCH:32/100 - Training Loss: 313.24035185675064, Validation Loss: 326.3268747965495\n",
            "EPOCH:33/100 - Training Loss: 313.0813008674858, Validation Loss: 326.96468157087054\n",
            "EPOCH:34/100 - Training Loss: 312.7428571386952, Validation Loss: 327.0580149332682\n",
            "EPOCH:35/100 - Training Loss: 310.5912445723868, Validation Loss: 326.80330723353796\n",
            "EPOCH:36/100 - Training Loss: 311.0925119258908, Validation Loss: 328.8313196091425\n",
            "EPOCH:37/100 - Training Loss: 309.87834429684005, Validation Loss: 327.30792149135044\n",
            "EPOCH:38/100 - Training Loss: 308.42736124480257, Validation Loss: 332.0785412016369\n",
            "EPOCH:39/100 - Training Loss: 308.2369814123914, Validation Loss: 330.0038580031622\n",
            "EPOCH:40/100 - Training Loss: 306.17576071072307, Validation Loss: 328.16761779785156\n",
            "EPOCH:41/100 - Training Loss: 305.98121592082384, Validation Loss: 327.82829037620905\n",
            "EPOCH:42/100 - Training Loss: 303.418040819555, Validation Loss: 332.2242848714193\n",
            "EPOCH:43/100 - Training Loss: 303.6794819979793, Validation Loss: 334.76289527529764\n",
            "EPOCH:44/100 - Training Loss: 302.4031177602691, Validation Loss: 330.7721362885975\n",
            "EPOCH:45/100 - Training Loss: 300.8330247282697, Validation Loss: 329.9699975876581\n",
            "EPOCH:46/100 - Training Loss: 300.16979212066724, Validation Loss: 328.1879573277065\n",
            "EPOCH:47/100 - Training Loss: 298.5003392622408, Validation Loss: 335.3194289434524\n",
            "EPOCH:48/100 - Training Loss: 298.65286639083826, Validation Loss: 331.08226434616813\n",
            "EPOCH:49/100 - Training Loss: 297.17121177072454, Validation Loss: 337.042957850865\n",
            "EPOCH:50/100 - Training Loss: 297.56598818103, Validation Loss: 329.2726951962426\n",
            "EPOCH:51/100 - Training Loss: 297.20928022798887, Validation Loss: 337.1757119315011\n",
            "EPOCH:52/100 - Training Loss: 294.05786078186765, Validation Loss: 331.7450943719773\n",
            "EPOCH:53/100 - Training Loss: 295.1744938306422, Validation Loss: 333.11522333054313\n",
            "EPOCH:54/100 - Training Loss: 293.1957677326794, Validation Loss: 333.66942531040735\n",
            "EPOCH:55/100 - Training Loss: 290.8339894916107, Validation Loss: 338.30552658807665\n",
            "EPOCH:56/100 - Training Loss: 291.2514846364752, Validation Loss: 331.16295107886907\n",
            "EPOCH:57/100 - Training Loss: 288.826861452089, Validation Loss: 336.45753406343005\n",
            "EPOCH:58/100 - Training Loss: 288.7537605267436, Validation Loss: 334.64078020368305\n",
            "EPOCH:59/100 - Training Loss: 289.90994666925764, Validation Loss: 333.72350071498323\n",
            "EPOCH:60/100 - Training Loss: 288.04304407889066, Validation Loss: 331.3870422363281\n",
            "EPOCH:61/100 - Training Loss: 286.847828102567, Validation Loss: 331.02154468354723\n",
            "EPOCH:62/100 - Training Loss: 285.9227906548038, Validation Loss: 331.24620143345425\n",
            "EPOCH:63/100 - Training Loss: 285.1392730312302, Validation Loss: 330.79986252557666\n",
            "EPOCH:64/100 - Training Loss: 285.27892702234675, Validation Loss: 337.0224398658389\n",
            "EPOCH:65/100 - Training Loss: 284.527468005342, Validation Loss: 339.9478265671503\n",
            "EPOCH:66/100 - Training Loss: 283.0032147245931, Validation Loss: 330.4598666236514\n",
            "EPOCH:67/100 - Training Loss: 282.5216142203757, Validation Loss: 338.2977053687686\n",
            "EPOCH:68/100 - Training Loss: 281.7519379754624, Validation Loss: 331.9449154808408\n",
            "EPOCH:69/100 - Training Loss: 281.9673400733238, Validation Loss: 332.6486832391648\n",
            "EPOCH:70/100 - Training Loss: 280.8403235460523, Validation Loss: 332.9657476515997\n",
            "EPOCH:71/100 - Training Loss: 280.67220579525167, Validation Loss: 331.9385936918713\n",
            "EPOCH:72/100 - Training Loss: 281.341187452075, Validation Loss: 337.2304888044085\n",
            "EPOCH:73/100 - Training Loss: 280.2050387944697, Validation Loss: 331.1417726062593\n",
            "EPOCH:74/100 - Training Loss: 279.1905930912865, Validation Loss: 334.24363723028273\n",
            "EPOCH:75/100 - Training Loss: 278.67225089790145, Validation Loss: 339.340676007952\n",
            "EPOCH:76/100 - Training Loss: 276.64034236643937, Validation Loss: 337.07611563546317\n",
            "EPOCH:77/100 - Training Loss: 277.62061892488975, Validation Loss: 332.5153315952846\n",
            "EPOCH:78/100 - Training Loss: 276.5278205962625, Validation Loss: 333.6372798374721\n",
            "EPOCH:79/100 - Training Loss: 276.9969106778894, Validation Loss: 332.8451493036179\n",
            "EPOCH:80/100 - Training Loss: 274.4711174065856, Validation Loss: 334.07406470889134\n",
            "EPOCH:81/100 - Training Loss: 273.36975419948095, Validation Loss: 337.20753464471727\n",
            "EPOCH:82/100 - Training Loss: 273.4994058358641, Validation Loss: 335.6535454159691\n",
            "EPOCH:83/100 - Training Loss: 273.27734697291845, Validation Loss: 333.6053276425316\n",
            "EPOCH:84/100 - Training Loss: 269.98670602584514, Validation Loss: 336.76942269461495\n",
            "EPOCH:85/100 - Training Loss: 271.3706687799786, Validation Loss: 337.747514125279\n",
            "EPOCH:86/100 - Training Loss: 270.58068479842956, Validation Loss: 340.4040273030599\n",
            "EPOCH:87/100 - Training Loss: 271.20095366339126, Validation Loss: 332.30286850702197\n",
            "EPOCH:88/100 - Training Loss: 268.98816016295075, Validation Loss: 335.1183138892764\n",
            "EPOCH:89/100 - Training Loss: 267.42162097211667, Validation Loss: 336.47693525041853\n",
            "EPOCH:90/100 - Training Loss: 266.7021001665575, Validation Loss: 336.8400682721819\n",
            "EPOCH:91/100 - Training Loss: 266.7803292467941, Validation Loss: 335.28330078125\n",
            "EPOCH:92/100 - Training Loss: 266.2353991415165, Validation Loss: 334.5566115606399\n",
            "EPOCH:93/100 - Training Loss: 264.58275667978074, Validation Loss: 335.0168467203776\n",
            "EPOCH:94/100 - Training Loss: 264.5057966828631, Validation Loss: 337.9145265125093\n",
            "EPOCH:95/100 - Training Loss: 262.28726025128424, Validation Loss: 334.8937460763114\n",
            "EPOCH:96/100 - Training Loss: 263.22979357589685, Validation Loss: 336.7276582263765\n",
            "EPOCH:97/100 - Training Loss: 262.5843670829099, Validation Loss: 336.4428126743862\n",
            "EPOCH:98/100 - Training Loss: 260.3296669206642, Validation Loss: 340.4796389625186\n",
            "EPOCH:99/100 - Training Loss: 260.29484235813624, Validation Loss: 335.4288311186291\n",
            "EPOCH:100/100 - Training Loss: 259.4370906347307, Validation Loss: 338.40070379348026\n",
            "Best epoch is epoch: 18\n",
            "Predictions saved\n",
            "\n",
            "\n",
            "Processing seed: 10/10\n",
            "\n",
            "\n",
            "EPOCH:1/100 - Training Loss: 403.58879504511066, Validation Loss: 344.4288614908854\n",
            "EPOCH:2/100 - Training Loss: 354.40268144630306, Validation Loss: 336.5471163795108\n",
            "EPOCH:3/100 - Training Loss: 345.6976817274435, Validation Loss: 332.3975472586496\n",
            "EPOCH:4/100 - Training Loss: 342.8490108225966, Validation Loss: 336.67488853817895\n",
            "EPOCH:5/100 - Training Loss: 340.66036047742017, Validation Loss: 330.0619272867838\n",
            "EPOCH:6/100 - Training Loss: 338.1488486497101, Validation Loss: 327.47310413178946\n",
            "EPOCH:7/100 - Training Loss: 335.6443793904525, Validation Loss: 327.67458103724886\n",
            "EPOCH:8/100 - Training Loss: 333.81097652462614, Validation Loss: 333.9150887625558\n",
            "EPOCH:9/100 - Training Loss: 332.4149917566122, Validation Loss: 331.0375790550595\n",
            "EPOCH:10/100 - Training Loss: 331.8090937939919, Validation Loss: 325.87688525971913\n",
            "EPOCH:11/100 - Training Loss: 330.0282765390765, Validation Loss: 327.1659500848679\n",
            "EPOCH:12/100 - Training Loss: 327.7868487738197, Validation Loss: 326.1751432872954\n",
            "EPOCH:13/100 - Training Loss: 327.31202570293857, Validation Loss: 327.6123266310919\n",
            "EPOCH:14/100 - Training Loss: 325.3087714657294, Validation Loss: 326.93221188499814\n",
            "EPOCH:15/100 - Training Loss: 323.8397168362055, Validation Loss: 327.1172864641462\n",
            "EPOCH:16/100 - Training Loss: 323.02949538492646, Validation Loss: 333.77833673386345\n",
            "EPOCH:17/100 - Training Loss: 322.90236676564365, Validation Loss: 327.99028451102123\n",
            "EPOCH:18/100 - Training Loss: 321.0295994287459, Validation Loss: 328.05057343982514\n",
            "EPOCH:19/100 - Training Loss: 320.70258806542734, Validation Loss: 327.5604532877604\n",
            "EPOCH:20/100 - Training Loss: 319.84182917701884, Validation Loss: 330.0253638857887\n",
            "EPOCH:21/100 - Training Loss: 318.003638547474, Validation Loss: 328.3958449590774\n",
            "EPOCH:22/100 - Training Loss: 317.65631948393684, Validation Loss: 335.2753362746466\n",
            "EPOCH:23/100 - Training Loss: 316.6199973386341, Validation Loss: 329.618458484468\n",
            "EPOCH:24/100 - Training Loss: 315.4909790694571, Validation Loss: 328.81503106980097\n",
            "EPOCH:25/100 - Training Loss: 314.74709755253394, Validation Loss: 329.5911250523159\n",
            "EPOCH:26/100 - Training Loss: 314.255101315446, Validation Loss: 330.2975158691406\n",
            "EPOCH:27/100 - Training Loss: 313.1742852695802, Validation Loss: 333.2538963681176\n",
            "EPOCH:28/100 - Training Loss: 313.20630643931094, Validation Loss: 330.22223220098584\n",
            "EPOCH:29/100 - Training Loss: 311.3691936119644, Validation Loss: 330.7426002139137\n",
            "EPOCH:30/100 - Training Loss: 311.1898771663839, Validation Loss: 330.41397850399926\n",
            "EPOCH:31/100 - Training Loss: 309.8820389267368, Validation Loss: 332.37988935198103\n",
            "EPOCH:32/100 - Training Loss: 309.5486780134761, Validation Loss: 330.14557466052827\n",
            "EPOCH:33/100 - Training Loss: 308.4149238750303, Validation Loss: 332.4814668201265\n",
            "EPOCH:34/100 - Training Loss: 307.539609449291, Validation Loss: 330.0739759172712\n",
            "EPOCH:35/100 - Training Loss: 305.80393646895743, Validation Loss: 334.8301753452846\n",
            "EPOCH:36/100 - Training Loss: 305.3004585575659, Validation Loss: 331.0270888555618\n",
            "EPOCH:37/100 - Training Loss: 304.0340620964979, Validation Loss: 331.57223249162945\n",
            "EPOCH:38/100 - Training Loss: 303.5169523689798, Validation Loss: 331.78912571498324\n",
            "EPOCH:39/100 - Training Loss: 302.0017687631393, Validation Loss: 338.7044203985305\n",
            "EPOCH:40/100 - Training Loss: 300.796773833136, Validation Loss: 332.35084097726\n",
            "EPOCH:41/100 - Training Loss: 299.86544018656656, Validation Loss: 331.80714256649924\n",
            "EPOCH:42/100 - Training Loss: 300.46780022231945, Validation Loss: 331.1460764567057\n",
            "EPOCH:43/100 - Training Loss: 297.8320490215729, Validation Loss: 331.80638834635414\n",
            "EPOCH:44/100 - Training Loss: 297.90064953278244, Validation Loss: 331.5685182117281\n",
            "EPOCH:45/100 - Training Loss: 296.3770767976674, Validation Loss: 331.85514657156807\n",
            "EPOCH:46/100 - Training Loss: 295.37008976538027, Validation Loss: 332.7454646519252\n",
            "EPOCH:47/100 - Training Loss: 294.5985318459304, Validation Loss: 332.54624851771763\n",
            "EPOCH:48/100 - Training Loss: 294.379713855097, Validation Loss: 332.2042912074498\n",
            "EPOCH:49/100 - Training Loss: 292.4636052753021, Validation Loss: 332.7545892624628\n",
            "EPOCH:50/100 - Training Loss: 291.35578485092856, Validation Loss: 334.1722997756231\n",
            "EPOCH:51/100 - Training Loss: 291.4214820406602, Validation Loss: 332.3516914004371\n",
            "EPOCH:52/100 - Training Loss: 289.4145270554718, Validation Loss: 344.80794343494233\n",
            "EPOCH:53/100 - Training Loss: 289.135707773286, Validation Loss: 333.35313836960563\n",
            "EPOCH:54/100 - Training Loss: 288.8456136844608, Validation Loss: 334.91895737420947\n",
            "EPOCH:55/100 - Training Loss: 287.99313529294545, Validation Loss: 339.87985462007066\n",
            "EPOCH:56/100 - Training Loss: 286.7670160079628, Validation Loss: 335.1071270170666\n",
            "EPOCH:57/100 - Training Loss: 287.2625928164097, Validation Loss: 334.4261484781901\n",
            "EPOCH:58/100 - Training Loss: 284.9891361609848, Validation Loss: 344.7634288969494\n",
            "EPOCH:59/100 - Training Loss: 285.1799828067315, Validation Loss: 341.0873089018322\n",
            "EPOCH:60/100 - Training Loss: 284.7768591650915, Validation Loss: 337.3285101027716\n",
            "EPOCH:61/100 - Training Loss: 282.5086782815064, Validation Loss: 345.440665108817\n",
            "EPOCH:62/100 - Training Loss: 283.53403325410903, Validation Loss: 334.8605936686198\n",
            "EPOCH:63/100 - Training Loss: 281.7478193405989, Validation Loss: 340.22864626929874\n",
            "EPOCH:64/100 - Training Loss: 279.9159012022906, Validation Loss: 335.71989193870905\n",
            "EPOCH:65/100 - Training Loss: 280.3662389787114, Validation Loss: 337.4841108049665\n",
            "EPOCH:66/100 - Training Loss: 279.7640009768538, Validation Loss: 335.81108136858256\n",
            "EPOCH:67/100 - Training Loss: 278.9665335607415, Validation Loss: 335.75173528762093\n",
            "EPOCH:68/100 - Training Loss: 279.7449661837557, Validation Loss: 336.58123168945315\n",
            "EPOCH:69/100 - Training Loss: 278.56748317818654, Validation Loss: 335.25154084705173\n",
            "EPOCH:70/100 - Training Loss: 277.0252136558223, Validation Loss: 335.62655174618675\n",
            "EPOCH:71/100 - Training Loss: 275.85262263987687, Validation Loss: 339.6150902157738\n",
            "EPOCH:72/100 - Training Loss: 277.4066654105175, Validation Loss: 338.68661019461496\n",
            "EPOCH:73/100 - Training Loss: 276.6153951203339, Validation Loss: 334.8536010742188\n",
            "EPOCH:74/100 - Training Loss: 274.9766592785965, Validation Loss: 336.0578823997861\n",
            "EPOCH:75/100 - Training Loss: 275.2863314316779, Validation Loss: 337.4031049455915\n",
            "EPOCH:76/100 - Training Loss: 274.3018638774717, Validation Loss: 342.7556857154483\n",
            "EPOCH:77/100 - Training Loss: 273.9273745370651, Validation Loss: 335.2317449660528\n",
            "EPOCH:78/100 - Training Loss: 273.3751578683785, Validation Loss: 337.0505908784412\n",
            "EPOCH:79/100 - Training Loss: 271.5974201211497, Validation Loss: 336.6582069033668\n",
            "EPOCH:80/100 - Training Loss: 272.4581525524932, Validation Loss: 337.9166564941406\n",
            "EPOCH:81/100 - Training Loss: 270.0933365742176, Validation Loss: 335.0097399030413\n",
            "EPOCH:82/100 - Training Loss: 270.86105694463544, Validation Loss: 344.00960954938614\n",
            "EPOCH:83/100 - Training Loss: 270.8167273218706, Validation Loss: 337.6327115013486\n",
            "EPOCH:84/100 - Training Loss: 270.86883699694795, Validation Loss: 345.0674326578776\n",
            "EPOCH:85/100 - Training Loss: 271.56403853386854, Validation Loss: 338.097072492327\n",
            "EPOCH:86/100 - Training Loss: 268.0618712395643, Validation Loss: 338.3744851248605\n",
            "EPOCH:87/100 - Training Loss: 267.3480159422663, Validation Loss: 338.1248321533203\n",
            "EPOCH:88/100 - Training Loss: 266.5545738420509, Validation Loss: 338.117714436849\n",
            "EPOCH:89/100 - Training Loss: 264.4546671937929, Validation Loss: 349.0746692475818\n",
            "EPOCH:90/100 - Training Loss: 267.42707764618723, Validation Loss: 338.1562244233631\n",
            "EPOCH:91/100 - Training Loss: 266.64410950289704, Validation Loss: 338.81930571056546\n",
            "EPOCH:92/100 - Training Loss: 264.9232902617898, Validation Loss: 342.2148825509208\n",
            "EPOCH:93/100 - Training Loss: 262.52115225052205, Validation Loss: 339.7213441394624\n",
            "EPOCH:94/100 - Training Loss: 262.68603930780597, Validation Loss: 346.47086007254467\n",
            "EPOCH:95/100 - Training Loss: 262.1498351905113, Validation Loss: 336.99391174316406\n",
            "EPOCH:96/100 - Training Loss: 262.1576550467771, Validation Loss: 341.441021437872\n",
            "EPOCH:97/100 - Training Loss: 260.4705037228532, Validation Loss: 338.5974965413412\n",
            "EPOCH:98/100 - Training Loss: 257.97962221496147, Validation Loss: 338.73590116954983\n",
            "EPOCH:99/100 - Training Loss: 258.961818212541, Validation Loss: 339.76013924734934\n",
            "EPOCH:100/100 - Training Loss: 258.92208425298793, Validation Loss: 339.36365850539437\n",
            "Best epoch is epoch: 10\n",
            "Predictions saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtest = pd.read_csv(RAW / 'test_examples.csv')\n",
        "\n",
        "files = [f'ff_{i}.csv' for i in range(1, 11)]\n",
        "ensemble(files=files, test_size=xtest.shape[0], file_name='ensemble.csv')\n"
      ],
      "metadata": {
        "id": "12ISMiCRqcVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "02s_BQmEbzBE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ensemble.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}